{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from model.unet import Unet3D\n",
    "from model.Diffusion import Diffusion_Control\n",
    "from model.EMA import ExponentialMovingAverage\n",
    "\n",
    "from dataset.SimpleShapeDataset.VoxelData import VoxelDataset, ConditionDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from utils.visualization import visualize_voxel_map\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxel_dataset_dir = \"dataset/SimpleShapeDataset/voxel_datasets\"\n",
    "\n",
    "voxel_dataset = VoxelDataset(voxel_dataset_dir)\n",
    "\n",
    "# Create the DataLoader\n",
    "batch_size = 2\n",
    "voxel_dataloader = DataLoader(voxel_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "condition_dataset_dir = \"dataset/SimpleShapeDataset/condition_datasets\"\n",
    "\n",
    "condition_dataset = ConditionDataset(voxel_dataset_dir)\n",
    "\n",
    "# Create the DataLoader\n",
    "batch_size = 2\n",
    "condition_dataloader = DataLoader(condition_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSELoss, self).__init__()\n",
    "        self.mse = nn.MSELoss(reduction='mean')\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        return torch.sqrt(self.mse(output, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Loading best checkpoint...\n",
      "Checkpoint loaded successfully!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'UnetWithControlNet' object has no attribute 'time_embedding'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 58\u001b[0m\n\u001b[1;32m     54\u001b[0m condition_batch \u001b[38;5;241m=\u001b[39m condition_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Model forward pass\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvoxel_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcondition_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Ensure model supports conditioning\u001b[39;00m\n\u001b[1;32m     59\u001b[0m noise \u001b[38;5;241m=\u001b[39m noise\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     60\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, noise)\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/diffusion_project/3d_generation/model/Diffusion.py:64\u001b[0m, in \u001b[0;36mDiffusion_Control.forward\u001b[0;34m(self, x, condition, noise)\u001b[0m\n\u001b[1;32m     61\u001b[0m x_t, c_t \u001b[38;5;241m=\u001b[39m x_t\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), c_t\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Predict the noise using the model\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m pred_noise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pred_noise\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/diffusion_project/3d_generation/model/control_net.py:78\u001b[0m, in \u001b[0;36mUnetWithControlNet.forward\u001b[0;34m(self, x, c, t)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, c, t\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;66;03m# Process input through the ControlNet\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m         t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime_embedding\u001b[49m(t)\n\u001b[1;32m     79\u001b[0m     control_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol_net(x, c, t)\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;66;03m# Process input through the original UNet encoder\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion/lib/python3.12/site-packages/torch/nn/modules/module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'UnetWithControlNet' object has no attribute 'time_embedding'"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "batch_size = 2\n",
    "model_ema_steps = 10\n",
    "num_epochs = 70\n",
    "model_ema_decay = 0.995\n",
    "\n",
    "# Define the model\n",
    "model = Diffusion_Control(\n",
    "    timesteps=1000,\n",
    "    image_size=64,\n",
    "    in_channels=1,\n",
    "    base_dim=32,\n",
    "    dim_mults=[1, 2, 4, 8]\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=100,\n",
    "    num_training_steps=(len(voxel_dataloader) * num_epochs),\n",
    ")\n",
    "adjust = 1 * batch_size * model_ema_steps / num_epochs\n",
    "alpha = 1.0 - model_ema_decay\n",
    "alpha = min(1.0, alpha * adjust)\n",
    "model_ema = ExponentialMovingAverage(model, device=device, decay=1.0 - alpha)\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "min_loss = np.inf\n",
    "global_steps = 0\n",
    "\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "checkpoint_path = \"results/steps_01244000.pt\"\n",
    "\n",
    "# Load checkpoint if exists\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(\"Loading best checkpoint...\")\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    unet_state_dict = {k.replace(\"model.\", \" \"): v for k, v in checkpoint['model'].items() if k.startswith(\"model.\")}\n",
    "    unet_state_dict = {k.strip(): v for k, v in unet_state_dict.items() if k.startswith(\" \")}\n",
    "\n",
    "    model.unet.load_state_dict(unet_state_dict)  # Load UNet parameters only\n",
    "    # model_ema.load_state_dict(ema_state_dict)  \n",
    "    print(\"Checkpoint loaded successfully!\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for (voxel_batch, condition_batch) in zip(voxel_dataloader, condition_dataloader):\n",
    "        # Prepare inputs\n",
    "        noise = torch.randn_like(voxel_batch).to(device)\n",
    "        voxel_batch = voxel_batch.to(device)\n",
    "        condition_batch = condition_batch.to(device)\n",
    "\n",
    "        # Model forward pass\n",
    "\n",
    "        pred = model(voxel_batch,condition_batch, noise)  # Ensure model supports conditioning\n",
    "        noise = noise.unsqueeze(1)\n",
    "        loss = loss_fn(pred, noise)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Update EMA\n",
    "        if global_steps % model_ema_steps == 0:\n",
    "            model_ema.update_parameters(model)\n",
    "\n",
    "        # Logging\n",
    "        if global_steps % 100 == 0:\n",
    "            print(f\"Epoch[{epoch + 1}/{num_epochs}], Step[{global_steps}], Loss: {loss.item():.4f}, lr: {lr_scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if loss.item() < min_loss and epoch > 1:\n",
    "            min_loss = loss.item()\n",
    "            torch.save(\n",
    "                {\"model\": model.state_dict(), \"model_ema\": model_ema.state_dict()},\n",
    "                f\"results/best.pt\"\n",
    "            )\n",
    "        global_steps += 1\n",
    "\n",
    "    # Save checkpoint\n",
    "    torch.save(\n",
    "        {\"model\": model.state_dict(), \"model_ema\": model_ema.state_dict()},\n",
    "        f\"condition_results/steps_{global_steps:08d}.pt\"\n",
    "    )\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 1000/1000 [00:32<00:00, 30.44it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:32<00:00, 31.01it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:31<00:00, 31.98it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:30<00:00, 32.65it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:31<00:00, 32.16it/s]\n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7ff0db29b860>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/zxj/anaconda3/envs/diffusion/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 770, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Paths and directories\n",
    "checkpoint_path = \"condition_results/best.pt\"\n",
    "save_dir = \"generated_samples\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Load the trained model and EMA\n",
    "model = Diffusion_Control(\n",
    "    timesteps=1000,\n",
    "    image_size=64,\n",
    "    in_channels=1,\n",
    "    base_dim=32,\n",
    "    dim_mults=[1, 2, 4, 8]\n",
    ").to(device)\n",
    "\n",
    "model_ema = ExponentialMovingAverage(model, device=device, decay=1.0 - 0.995)\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(\"Loading best checkpoint...\")\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "    model_ema.load_state_dict(checkpoint[\"model_ema\"])\n",
    "    print(\"Checkpoint loaded successfully!\")\n",
    "\n",
    "model.eval()  # Switch to evaluation mode\n",
    "model_ema.eval()\n",
    "\n",
    "# Data loader for control dataset\n",
    "# control_dataset_loader = ...  # Define your control dataset loader here\n",
    "condition_dataloader = DataLoader(condition_dataset, batch_size=batch_size, shuffle=True)\n",
    "samples = []\n",
    "with torch.no_grad():\n",
    "    for i, condition_batch in enumerate(condition_dataloader):\n",
    "        condition_batch = condition_batch.to(device)\n",
    "\n",
    "        # Generate noise for sampling\n",
    "        batch_size = condition_batch.size(0)\n",
    "        noise = torch.randn(batch_size, 1, 64, 64).to(device)\n",
    "\n",
    "        # Perform sampling with the model\n",
    "        generated_samples = model_ema.module.sampling(batch_size, condition_batch, noise)\n",
    "        \n",
    "        # Process samples\n",
    "        for j in range(batch_size):\n",
    "            voxel_1d_array = generated_samples[j].cpu().numpy()\n",
    "\n",
    "            # Convert to binary voxel data\n",
    "            binary_data = (voxel_1d_array > 0.5).astype(int)\n",
    "\n",
    "            # Parameters for voxel map\n",
    "            voxel_size = 0.25\n",
    "            grid_size = 64\n",
    "\n",
    "            # Reshape binary data into 8x8x8\n",
    "            reshaped_data = binary_data.reshape(grid_size, 1, grid_size, 1, grid_size, 1).mean(axis=(1, 3, 5))\n",
    "            voxel_data = (reshaped_data > 0.5).astype(int)\n",
    "\n",
    "            # Prepare the 3D plot\n",
    "            fig = plt.figure(figsize=(10, 10))\n",
    "            ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "            # Create a 3D grid for the voxel map\n",
    "            x, y, z = np.indices((grid_size + 1, grid_size + 1, grid_size + 1)) * voxel_size\n",
    "\n",
    "            # Display voxels\n",
    "            filled_voxels = (voxel_data == 1)\n",
    "            ax.voxels(x, y, z, filled_voxels, facecolors=\"blue\", edgecolors=\"black\", alpha=0.7)\n",
    "\n",
    "            # Set labels and aspect ratio\n",
    "            ax.set_xlabel('X')\n",
    "            ax.set_ylabel('Y')\n",
    "            ax.set_zlabel('Z')\n",
    "            ax.set_aspect('auto')\n",
    "            plt.title(\"8x8x8 Voxel Map\")\n",
    "\n",
    "            # Save the plot\n",
    "            save_path = os.path.join(save_dir, f\"sample_{i * batch_size + j + 1}.png\")\n",
    "            plt.savefig(save_path)\n",
    "            plt.close(fig)\n",
    "\n",
    "        # Break loop after generating desired number of samples\n",
    "        if len(samples) >= 2:\n",
    "            break\n",
    "\n",
    "print(\"Generation complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
