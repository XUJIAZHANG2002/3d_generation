{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from model.unet import Unet3D\n",
    "from model.Diffusion import Diffusion_Control\n",
    "from model.EMA import ExponentialMovingAverage\n",
    "\n",
    "from dataset.SimpleShapeDataset.VoxelData import VoxelDataset, ConditionDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from utils.visualization import visualize_voxel_map\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxel_dataset_dir = \"dataset/SimpleShapeDataset/voxel_datasets\"\n",
    "\n",
    "voxel_dataset = VoxelDataset(voxel_dataset_dir)\n",
    "\n",
    "# Create the DataLoader\n",
    "batch_size = 2\n",
    "voxel_dataloader = DataLoader(voxel_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "condition_dataset_dir = \"dataset/SimpleShapeDataset/condition_datasets\"\n",
    "\n",
    "condition_dataset = ConditionDataset(voxel_dataset_dir)\n",
    "\n",
    "# Create the DataLoader\n",
    "batch_size = 2\n",
    "condition_dataloader = DataLoader(condition_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSELoss, self).__init__()\n",
    "        self.mse = nn.MSELoss(reduction='mean')\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        return torch.sqrt(self.mse(output, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Loading best checkpoint...\n",
      "Checkpoint loaded successfully!\n",
      "Epoch[1/70], Step[1074000], Loss: 0.1951, lr: 0.000001\n",
      "Epoch[1/70], Step[1074100], Loss: 0.0522, lr: 0.000100\n",
      "Epoch[1/70], Step[1074200], Loss: 0.1357, lr: 0.000100\n",
      "Epoch[1/70], Step[1074300], Loss: 0.3089, lr: 0.000100\n",
      "Epoch[1/70], Step[1074400], Loss: 0.1186, lr: 0.000100\n",
      "Epoch[1/70], Step[1074500], Loss: 0.0588, lr: 0.000100\n",
      "Epoch[1/70], Step[1074600], Loss: 0.0775, lr: 0.000100\n",
      "Epoch[1/70], Step[1074700], Loss: 0.0315, lr: 0.000100\n",
      "Epoch[1/70], Step[1074800], Loss: 0.0510, lr: 0.000100\n",
      "Epoch[1/70], Step[1074900], Loss: 0.0799, lr: 0.000100\n",
      "Epoch[1/70], Step[1075000], Loss: 0.1750, lr: 0.000100\n",
      "Epoch[1/70], Step[1075100], Loss: 0.0313, lr: 0.000100\n",
      "Epoch[1/70], Step[1075200], Loss: 0.0396, lr: 0.000100\n",
      "Epoch[1/70], Step[1075300], Loss: 0.1378, lr: 0.000100\n",
      "Epoch[1/70], Step[1075400], Loss: 0.1062, lr: 0.000100\n",
      "Epoch[1/70], Step[1075500], Loss: 0.0733, lr: 0.000100\n",
      "Epoch[1/70], Step[1075600], Loss: 0.0884, lr: 0.000100\n",
      "Epoch[1/70], Step[1075700], Loss: 0.2020, lr: 0.000100\n",
      "Epoch[1/70], Step[1075800], Loss: 0.1091, lr: 0.000100\n",
      "Epoch[1/70], Step[1075900], Loss: 0.0214, lr: 0.000100\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory condition_results does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 86\u001b[0m\n\u001b[1;32m     83\u001b[0m         global_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;66;03m# Save checkpoint\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_ema\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_ema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcondition_results/steps_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mglobal_steps\u001b[49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[38;5;124;43m08d\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion/lib/python3.12/site-packages/torch/serialization.py:628\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    625\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 628\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    629\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[1;32m    630\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion/lib/python3.12/site-packages/torch/serialization.py:502\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    501\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[0;32m--> 502\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion/lib/python3.12/site-packages/torch/serialization.py:473\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream))\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 473\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Parent directory condition_results does not exist."
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "batch_size = 2\n",
    "model_ema_steps = 10\n",
    "num_epochs = 70\n",
    "model_ema_decay = 0.995\n",
    "\n",
    "# Define the model\n",
    "model = Diffusion_Control(\n",
    "    timesteps=1000,\n",
    "    image_size=64,\n",
    "    in_channels=1,\n",
    "    base_dim=32,\n",
    "    dim_mults=[1, 2, 4, 8]\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=100,\n",
    "    num_training_steps=(len(voxel_dataloader) * num_epochs),\n",
    ")\n",
    "adjust = 1 * batch_size * model_ema_steps / num_epochs\n",
    "alpha = 1.0 - model_ema_decay\n",
    "alpha = min(1.0, alpha * adjust)\n",
    "model_ema = ExponentialMovingAverage(model, device=device, decay=1.0 - alpha)\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "min_loss = np.inf\n",
    "global_steps = 0\n",
    "\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "checkpoint_path = \"results/steps_01244000.pt\"\n",
    "\n",
    "# Load checkpoint if exists\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(\"Loading best checkpoint...\")\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    unet_state_dict = {k.replace(\"model.\", \" \"): v for k, v in checkpoint['model'].items() if k.startswith(\"model.\")}\n",
    "    unet_state_dict = {k.strip(): v for k, v in unet_state_dict.items() if k.startswith(\" \")}\n",
    "\n",
    "    model.unet.load_state_dict(unet_state_dict)  # Load UNet parameters only\n",
    "    # model_ema.load_state_dict(ema_state_dict)  \n",
    "    print(\"Checkpoint loaded successfully!\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for (voxel_batch, condition_batch) in zip(voxel_dataloader, condition_dataloader):\n",
    "        # Prepare inputs\n",
    "        noise = torch.randn_like(voxel_batch).to(device)\n",
    "        voxel_batch = voxel_batch.to(device)\n",
    "        condition_batch = condition_batch.to(device)\n",
    "\n",
    "        # Model forward pass\n",
    "\n",
    "        pred = model(voxel_batch,condition_batch, noise)  # Ensure model supports conditioning\n",
    "        noise = noise.unsqueeze(1)\n",
    "        loss = loss_fn(pred, noise)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Update EMA\n",
    "        if global_steps % model_ema_steps == 0:\n",
    "            model_ema.update_parameters(model)\n",
    "\n",
    "        # Logging\n",
    "        if global_steps % 100 == 0:\n",
    "            print(f\"Epoch[{epoch + 1}/{num_epochs}], Step[{global_steps}], Loss: {loss.item():.4f}, lr: {lr_scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if loss.item() < min_loss and epoch > 1:\n",
    "            min_loss = loss.item()\n",
    "            torch.save(\n",
    "                {\"model\": model.state_dict(), \"model_ema\": model_ema.state_dict()},\n",
    "                f\"results/best.pt\"\n",
    "            )\n",
    "        global_steps += 1\n",
    "\n",
    "    # Save checkpoint\n",
    "    torch.save(\n",
    "        {\"model\": model.state_dict(), \"model_ema\": model_ema.state_dict()},\n",
    "        f\"condition_results/steps_{global_steps:08d}.pt\"\n",
    "    )\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "print(device)\n",
    "batch_size = 2; model_ema_steps = 10;num_epochs = 20; model_ema_decay = 0.995\n",
    "\n",
    "# Define the model\n",
    "model = Diffusion(timesteps=1000,\n",
    "                        image_size=64,\n",
    "                        in_channels=1,\n",
    "                        base_dim=32,\n",
    "                        dim_mults=[1,2, 4,8]).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "optimizer=optimizer,\n",
    "num_warmup_steps=100,\n",
    "num_training_steps=(len(voxel_dataloader) * num_epochs),\n",
    ")\n",
    "adjust = 1* batch_size * model_ema_steps / num_epochs\n",
    "alpha = 1.0 - model_ema_decay\n",
    "alpha = min(1.0, alpha * adjust)\n",
    "model_ema = ExponentialMovingAverage(model, device=device, decay=1.0 - alpha)\n",
    "# Example usage:\n",
    "# loss_fn = RMSELoss()\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "min_loss = np.inf\n",
    "global_steps = 40000\n",
    "\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "checkpoint_path = \"results/steps_01174000.pt\"\n",
    "\n",
    "# if os.path.exists(checkpoint_path):\n",
    "#     print(\"Loading best checkpoint...\")\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "save_dir = \"output_voxel_maps\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "model_ema.load_state_dict(checkpoint[\"model_ema\"])\n",
    "# model_ema = ExponentialMovingAverage(model, device=device, decay=1.0 - 0.1)\n",
    "samples = []\n",
    "for i in range(32):\n",
    "        samples.append(model_ema.module.sampling(1, device=device))\n",
    "\n",
    "for i in range(32):\n",
    "        voxel_1d_array = samples[i].cpu().numpy()\n",
    "        # voxel_1d_array += 1\n",
    "        # voxel_1d_array *=0.5\n",
    "        binary_data = (voxel_1d_array > 0.5).astype(int)\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        from mpl_toolkits.mplot3d import Axes3D\n",
    "        # print(binary_data)\n",
    "        # Parameters for voxel map\n",
    "        voxel_size = 0.25\n",
    "        grid_size = 64\n",
    "\n",
    "        # Reshape binary data into 8x8x8 by averaging blocks of 4x4x4\n",
    "        reshaped_data = binary_data.reshape(grid_size, 1,grid_size, 1, grid_size, 1).mean(axis=(1, 3, 5))\n",
    "        voxel_data = (reshaped_data > 0.5).astype(int)  # Convert to binary based on average\n",
    "\n",
    "        # Prepare the 3D plot\n",
    "        fig = plt.figure(figsize=(10, 10))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "        # Create a 3D grid for the voxel map (dimensions + 1 to align with voxel corners)\n",
    "        x, y, z = np.indices((grid_size + 1, grid_size + 1, grid_size + 1)) * voxel_size\n",
    "\n",
    "        # Display voxels\n",
    "        filled_voxels = (voxel_data == 1)\n",
    "\n",
    "        ax.voxels(x, y, z, filled_voxels, \n",
    "                facecolors=\"blue\", edgecolors=\"black\", alpha=0.7)\n",
    "\n",
    "        # Set labels and aspect ratio\n",
    "        ax.set_xlabel('X')\n",
    "        ax.set_ylabel('Y')\n",
    "        ax.set_zlabel('Z')\n",
    "        ax.set_aspect('auto')\n",
    "        plt.title(\"8x8x8 Voxel Map\")\n",
    "\n",
    "        # Show plot\n",
    "        # plt.show()\n",
    "        save_path = os.path.join(save_dir, f\"sample_{i+33}.png\")\n",
    "        plt.savefig(save_path)\n",
    "        plt.close(fig)\n",
    "\n",
    "    # plt.savefig(save_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
