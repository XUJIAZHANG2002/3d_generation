{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from model.unet import Unet3D\n",
    "from model.Diffusion import Diffusion_Control\n",
    "from model.EMA import ExponentialMovingAverage\n",
    "\n",
    "from dataset.SimpleShapeDataset.VoxelData import VoxelDataset, ConditionDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from utils.visualization import visualize_voxel_map\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxel_dataset_dir = \"dataset/SimpleShapeDataset/voxel_datasets\"\n",
    "\n",
    "voxel_dataset = VoxelDataset(voxel_dataset_dir)\n",
    "\n",
    "# Create the DataLoader\n",
    "batch_size = 2\n",
    "voxel_dataloader = DataLoader(voxel_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "condition_dataset_dir = \"dataset/SimpleShapeDataset/condition_datasets\"\n",
    "\n",
    "condition_dataset = ConditionDataset(voxel_dataset_dir)\n",
    "\n",
    "# Create the DataLoader\n",
    "batch_size = 2\n",
    "condition_dataloader = DataLoader(condition_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSELoss, self).__init__()\n",
    "        self.mse = nn.MSELoss(reduction='mean')\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        return torch.sqrt(self.mse(output, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Loading best checkpoint...\n",
      "Checkpoint loaded successfully!\n",
      "Epoch[1/70], Step[0], Loss: 0.0722, lr: 0.000000\n",
      "Epoch[1/70], Step[100], Loss: 0.0067, lr: 0.000010\n",
      "Epoch[1/70], Step[200], Loss: 0.0114, lr: 0.000010\n",
      "Epoch[1/70], Step[300], Loss: 0.0054, lr: 0.000010\n",
      "Epoch[1/70], Step[400], Loss: 0.0219, lr: 0.000010\n",
      "Epoch[1/70], Step[500], Loss: 0.0033, lr: 0.000010\n",
      "Epoch[1/70], Step[600], Loss: 0.0018, lr: 0.000010\n",
      "Epoch[1/70], Step[700], Loss: 0.0827, lr: 0.000010\n",
      "Epoch[1/70], Step[800], Loss: 0.0825, lr: 0.000010\n",
      "Epoch[1/70], Step[900], Loss: 0.0106, lr: 0.000010\n",
      "Epoch[1/70], Step[1000], Loss: 0.0145, lr: 0.000010\n",
      "Epoch[1/70], Step[1100], Loss: 0.0243, lr: 0.000010\n",
      "Epoch[1/70], Step[1200], Loss: 0.0119, lr: 0.000010\n",
      "Epoch[1/70], Step[1300], Loss: 0.0045, lr: 0.000010\n",
      "Epoch[1/70], Step[1400], Loss: 0.0064, lr: 0.000010\n",
      "Epoch[1/70], Step[1500], Loss: 0.0029, lr: 0.000010\n",
      "Epoch[1/70], Step[1600], Loss: 0.0153, lr: 0.000010\n",
      "Epoch[1/70], Step[1700], Loss: 0.0762, lr: 0.000010\n",
      "Epoch[1/70], Step[1800], Loss: 0.0153, lr: 0.000010\n",
      "Epoch[1/70], Step[1900], Loss: 0.0084, lr: 0.000010\n",
      "Epoch[2/70], Step[2000], Loss: 0.0105, lr: 0.000010\n",
      "Epoch[2/70], Step[2100], Loss: 0.0035, lr: 0.000010\n",
      "Epoch[2/70], Step[2200], Loss: 0.0249, lr: 0.000010\n",
      "Epoch[2/70], Step[2300], Loss: 0.0101, lr: 0.000010\n",
      "Epoch[2/70], Step[2400], Loss: 0.0012, lr: 0.000010\n",
      "Epoch[2/70], Step[2500], Loss: 0.0175, lr: 0.000010\n",
      "Epoch[2/70], Step[2600], Loss: 0.0022, lr: 0.000010\n",
      "Epoch[2/70], Step[2700], Loss: 0.0013, lr: 0.000010\n",
      "Epoch[2/70], Step[2800], Loss: 0.0482, lr: 0.000010\n",
      "Epoch[2/70], Step[2900], Loss: 0.0059, lr: 0.000010\n",
      "Epoch[2/70], Step[3000], Loss: 0.0256, lr: 0.000010\n",
      "Epoch[2/70], Step[3100], Loss: 0.0116, lr: 0.000010\n",
      "Epoch[2/70], Step[3200], Loss: 0.0785, lr: 0.000010\n",
      "Epoch[2/70], Step[3300], Loss: 0.0162, lr: 0.000010\n",
      "Epoch[2/70], Step[3400], Loss: 0.0062, lr: 0.000010\n",
      "Epoch[2/70], Step[3500], Loss: 0.0154, lr: 0.000010\n",
      "Epoch[2/70], Step[3600], Loss: 0.0027, lr: 0.000010\n",
      "Epoch[2/70], Step[3700], Loss: 0.0556, lr: 0.000010\n",
      "Epoch[2/70], Step[3800], Loss: 0.0165, lr: 0.000010\n",
      "Epoch[2/70], Step[3900], Loss: 0.0068, lr: 0.000010\n",
      "Epoch[3/70], Step[4000], Loss: 0.0260, lr: 0.000010\n",
      "Epoch[3/70], Step[4100], Loss: 0.0183, lr: 0.000010\n",
      "Epoch[3/70], Step[4200], Loss: 0.0064, lr: 0.000010\n",
      "Epoch[3/70], Step[4300], Loss: 0.0054, lr: 0.000010\n",
      "Epoch[3/70], Step[4400], Loss: 0.0029, lr: 0.000010\n",
      "Epoch[3/70], Step[4500], Loss: 0.0231, lr: 0.000010\n",
      "Epoch[3/70], Step[4600], Loss: 0.0665, lr: 0.000010\n",
      "Epoch[3/70], Step[4700], Loss: 0.0010, lr: 0.000010\n",
      "Epoch[3/70], Step[4800], Loss: 0.0659, lr: 0.000010\n",
      "Epoch[3/70], Step[4900], Loss: 0.0119, lr: 0.000010\n",
      "Epoch[3/70], Step[5000], Loss: 0.0075, lr: 0.000010\n",
      "Epoch[3/70], Step[5100], Loss: 0.0130, lr: 0.000010\n",
      "Epoch[3/70], Step[5200], Loss: 0.0017, lr: 0.000010\n",
      "Epoch[3/70], Step[5300], Loss: 0.0023, lr: 0.000010\n",
      "Epoch[3/70], Step[5400], Loss: 0.0057, lr: 0.000010\n",
      "Epoch[3/70], Step[5500], Loss: 0.0058, lr: 0.000010\n",
      "Epoch[3/70], Step[5600], Loss: 0.0050, lr: 0.000010\n",
      "Epoch[3/70], Step[5700], Loss: 0.1022, lr: 0.000010\n",
      "Epoch[3/70], Step[5800], Loss: 0.0411, lr: 0.000010\n",
      "Epoch[3/70], Step[5900], Loss: 0.0312, lr: 0.000010\n",
      "Epoch[4/70], Step[6000], Loss: 0.0436, lr: 0.000010\n",
      "Epoch[4/70], Step[6100], Loss: 0.0048, lr: 0.000010\n",
      "Epoch[4/70], Step[6200], Loss: 0.0181, lr: 0.000010\n",
      "Epoch[4/70], Step[6300], Loss: 0.0087, lr: 0.000010\n",
      "Epoch[4/70], Step[6400], Loss: 0.0226, lr: 0.000010\n",
      "Epoch[4/70], Step[6500], Loss: 0.0191, lr: 0.000010\n",
      "Epoch[4/70], Step[6600], Loss: 0.0230, lr: 0.000010\n",
      "Epoch[4/70], Step[6700], Loss: 0.0552, lr: 0.000010\n",
      "Epoch[4/70], Step[6800], Loss: 0.0743, lr: 0.000010\n",
      "Epoch[4/70], Step[6900], Loss: 0.0146, lr: 0.000010\n",
      "Epoch[4/70], Step[7000], Loss: 0.0023, lr: 0.000010\n",
      "Epoch[4/70], Step[7100], Loss: 0.0579, lr: 0.000010\n",
      "Epoch[4/70], Step[7200], Loss: 0.0739, lr: 0.000010\n",
      "Epoch[4/70], Step[7300], Loss: 0.0035, lr: 0.000010\n",
      "Epoch[4/70], Step[7400], Loss: 0.0127, lr: 0.000010\n",
      "Epoch[4/70], Step[7500], Loss: 0.0138, lr: 0.000010\n",
      "Epoch[4/70], Step[7600], Loss: 0.0007, lr: 0.000010\n",
      "Epoch[4/70], Step[7700], Loss: 0.0174, lr: 0.000010\n",
      "Epoch[4/70], Step[7800], Loss: 0.0874, lr: 0.000010\n",
      "Epoch[4/70], Step[7900], Loss: 0.0752, lr: 0.000010\n",
      "Epoch[5/70], Step[8000], Loss: 0.0075, lr: 0.000010\n",
      "Epoch[5/70], Step[8100], Loss: 0.0044, lr: 0.000010\n",
      "Epoch[5/70], Step[8200], Loss: 0.0018, lr: 0.000010\n",
      "Epoch[5/70], Step[8300], Loss: 0.0065, lr: 0.000010\n",
      "Epoch[5/70], Step[8400], Loss: 0.0169, lr: 0.000010\n",
      "Epoch[5/70], Step[8500], Loss: 0.0033, lr: 0.000010\n",
      "Epoch[5/70], Step[8600], Loss: 0.0014, lr: 0.000010\n",
      "Epoch[5/70], Step[8700], Loss: 0.0440, lr: 0.000010\n",
      "Epoch[5/70], Step[8800], Loss: 0.0701, lr: 0.000010\n",
      "Epoch[5/70], Step[8900], Loss: 0.0195, lr: 0.000010\n",
      "Epoch[5/70], Step[9000], Loss: 0.0035, lr: 0.000010\n",
      "Epoch[5/70], Step[9100], Loss: 0.0254, lr: 0.000010\n",
      "Epoch[5/70], Step[9200], Loss: 0.0786, lr: 0.000010\n",
      "Epoch[5/70], Step[9300], Loss: 0.0103, lr: 0.000010\n",
      "Epoch[5/70], Step[9400], Loss: 0.0067, lr: 0.000010\n",
      "Epoch[5/70], Step[9500], Loss: 0.0066, lr: 0.000010\n",
      "Epoch[5/70], Step[9600], Loss: 0.0137, lr: 0.000010\n",
      "Epoch[5/70], Step[9700], Loss: 0.0269, lr: 0.000010\n",
      "Epoch[5/70], Step[9800], Loss: 0.0119, lr: 0.000010\n",
      "Epoch[5/70], Step[9900], Loss: 0.0425, lr: 0.000010\n",
      "Epoch[6/70], Step[10000], Loss: 0.0059, lr: 0.000010\n",
      "Epoch[6/70], Step[10100], Loss: 0.0127, lr: 0.000010\n",
      "Epoch[6/70], Step[10200], Loss: 0.0189, lr: 0.000010\n",
      "Epoch[6/70], Step[10300], Loss: 0.0080, lr: 0.000010\n",
      "Epoch[6/70], Step[10400], Loss: 0.0041, lr: 0.000010\n",
      "Epoch[6/70], Step[10500], Loss: 0.0225, lr: 0.000010\n",
      "Epoch[6/70], Step[10600], Loss: 0.0620, lr: 0.000010\n",
      "Epoch[6/70], Step[10700], Loss: 0.0085, lr: 0.000010\n",
      "Epoch[6/70], Step[10800], Loss: 0.1621, lr: 0.000010\n",
      "Epoch[6/70], Step[10900], Loss: 0.0033, lr: 0.000010\n",
      "Epoch[6/70], Step[11000], Loss: 0.0114, lr: 0.000010\n",
      "Epoch[6/70], Step[11100], Loss: 0.0587, lr: 0.000010\n",
      "Epoch[6/70], Step[11200], Loss: 0.0081, lr: 0.000010\n",
      "Epoch[6/70], Step[11300], Loss: 0.0018, lr: 0.000010\n",
      "Epoch[6/70], Step[11400], Loss: 0.0015, lr: 0.000010\n",
      "Epoch[6/70], Step[11500], Loss: 0.0023, lr: 0.000010\n",
      "Epoch[6/70], Step[11600], Loss: 0.0146, lr: 0.000010\n",
      "Epoch[6/70], Step[11700], Loss: 0.0632, lr: 0.000010\n",
      "Epoch[6/70], Step[11800], Loss: 0.0055, lr: 0.000010\n",
      "Epoch[6/70], Step[11900], Loss: 0.0065, lr: 0.000010\n",
      "Epoch[7/70], Step[12000], Loss: 0.0136, lr: 0.000010\n",
      "Epoch[7/70], Step[12100], Loss: 0.0031, lr: 0.000010\n",
      "Epoch[7/70], Step[12200], Loss: 0.0052, lr: 0.000010\n",
      "Epoch[7/70], Step[12300], Loss: 0.0132, lr: 0.000010\n",
      "Epoch[7/70], Step[12400], Loss: 0.0006, lr: 0.000010\n",
      "Epoch[7/70], Step[12500], Loss: 0.0184, lr: 0.000010\n",
      "Epoch[7/70], Step[12600], Loss: 0.0040, lr: 0.000010\n",
      "Epoch[7/70], Step[12700], Loss: 0.0355, lr: 0.000010\n",
      "Epoch[7/70], Step[12800], Loss: 0.0269, lr: 0.000010\n",
      "Epoch[7/70], Step[12900], Loss: 0.0033, lr: 0.000010\n",
      "Epoch[7/70], Step[13000], Loss: 0.0027, lr: 0.000010\n",
      "Epoch[7/70], Step[13100], Loss: 0.0780, lr: 0.000010\n",
      "Epoch[7/70], Step[13200], Loss: 0.0084, lr: 0.000010\n",
      "Epoch[7/70], Step[13300], Loss: 0.0035, lr: 0.000010\n",
      "Epoch[7/70], Step[13400], Loss: 0.0020, lr: 0.000010\n",
      "Epoch[7/70], Step[13500], Loss: 0.0034, lr: 0.000010\n",
      "Epoch[7/70], Step[13600], Loss: 0.0175, lr: 0.000010\n",
      "Epoch[7/70], Step[13700], Loss: 0.0026, lr: 0.000010\n",
      "Epoch[7/70], Step[13800], Loss: 0.0493, lr: 0.000010\n",
      "Epoch[7/70], Step[13900], Loss: 0.0080, lr: 0.000010\n",
      "Epoch[8/70], Step[14000], Loss: 0.0810, lr: 0.000010\n",
      "Epoch[8/70], Step[14100], Loss: 0.0070, lr: 0.000010\n",
      "Epoch[8/70], Step[14200], Loss: 0.0097, lr: 0.000010\n",
      "Epoch[8/70], Step[14300], Loss: 0.0128, lr: 0.000010\n",
      "Epoch[8/70], Step[14400], Loss: 0.0026, lr: 0.000010\n",
      "Epoch[8/70], Step[14500], Loss: 0.0058, lr: 0.000010\n",
      "Epoch[8/70], Step[14600], Loss: 0.0011, lr: 0.000010\n",
      "Epoch[8/70], Step[14700], Loss: 0.0181, lr: 0.000010\n",
      "Epoch[8/70], Step[14800], Loss: 0.0160, lr: 0.000010\n",
      "Epoch[8/70], Step[14900], Loss: 0.0202, lr: 0.000010\n",
      "Epoch[8/70], Step[15000], Loss: 0.0061, lr: 0.000010\n",
      "Epoch[8/70], Step[15100], Loss: 0.0843, lr: 0.000010\n",
      "Epoch[8/70], Step[15200], Loss: 0.0041, lr: 0.000010\n",
      "Epoch[8/70], Step[15300], Loss: 0.0100, lr: 0.000010\n",
      "Epoch[8/70], Step[15400], Loss: 0.0168, lr: 0.000010\n",
      "Epoch[8/70], Step[15500], Loss: 0.0104, lr: 0.000010\n",
      "Epoch[8/70], Step[15600], Loss: 0.0027, lr: 0.000010\n",
      "Epoch[8/70], Step[15700], Loss: 0.1038, lr: 0.000010\n",
      "Epoch[8/70], Step[15800], Loss: 0.0044, lr: 0.000010\n",
      "Epoch[8/70], Step[15900], Loss: 0.0513, lr: 0.000010\n",
      "Epoch[9/70], Step[16000], Loss: 0.0795, lr: 0.000010\n",
      "Epoch[9/70], Step[16100], Loss: 0.0160, lr: 0.000010\n",
      "Epoch[9/70], Step[16200], Loss: 0.0187, lr: 0.000010\n",
      "Epoch[9/70], Step[16300], Loss: 0.0016, lr: 0.000010\n",
      "Epoch[9/70], Step[16400], Loss: 0.0018, lr: 0.000010\n",
      "Epoch[9/70], Step[16500], Loss: 0.0004, lr: 0.000010\n",
      "Epoch[9/70], Step[16600], Loss: 0.0022, lr: 0.000010\n",
      "Epoch[9/70], Step[16700], Loss: 0.0020, lr: 0.000010\n",
      "Epoch[9/70], Step[16800], Loss: 0.0840, lr: 0.000010\n",
      "Epoch[9/70], Step[16900], Loss: 0.0140, lr: 0.000010\n",
      "Epoch[9/70], Step[17000], Loss: 0.0240, lr: 0.000010\n",
      "Epoch[9/70], Step[17100], Loss: 0.0815, lr: 0.000010\n",
      "Epoch[9/70], Step[17200], Loss: 0.0683, lr: 0.000010\n",
      "Epoch[9/70], Step[17300], Loss: 0.0110, lr: 0.000010\n",
      "Epoch[9/70], Step[17400], Loss: 0.0119, lr: 0.000010\n",
      "Epoch[9/70], Step[17500], Loss: 0.0151, lr: 0.000010\n",
      "Epoch[9/70], Step[17600], Loss: 0.0148, lr: 0.000010\n",
      "Epoch[9/70], Step[17700], Loss: 0.0617, lr: 0.000010\n",
      "Epoch[9/70], Step[17800], Loss: 0.0139, lr: 0.000010\n",
      "Epoch[9/70], Step[17900], Loss: 0.0116, lr: 0.000010\n",
      "Epoch[10/70], Step[18000], Loss: 0.0218, lr: 0.000010\n",
      "Epoch[10/70], Step[18100], Loss: 0.0155, lr: 0.000010\n",
      "Epoch[10/70], Step[18200], Loss: 0.0238, lr: 0.000010\n",
      "Epoch[10/70], Step[18300], Loss: 0.0011, lr: 0.000010\n",
      "Epoch[10/70], Step[18400], Loss: 0.0053, lr: 0.000010\n",
      "Epoch[10/70], Step[18500], Loss: 0.0171, lr: 0.000010\n",
      "Epoch[10/70], Step[18600], Loss: 0.0037, lr: 0.000010\n",
      "Epoch[10/70], Step[18700], Loss: 0.0023, lr: 0.000010\n",
      "Epoch[10/70], Step[18800], Loss: 0.0206, lr: 0.000010\n",
      "Epoch[10/70], Step[18900], Loss: 0.0185, lr: 0.000010\n",
      "Epoch[10/70], Step[19000], Loss: 0.0060, lr: 0.000010\n",
      "Epoch[10/70], Step[19100], Loss: 0.0138, lr: 0.000010\n",
      "Epoch[10/70], Step[19200], Loss: 0.0370, lr: 0.000010\n",
      "Epoch[10/70], Step[19300], Loss: 0.0011, lr: 0.000010\n",
      "Epoch[10/70], Step[19400], Loss: 0.0027, lr: 0.000010\n",
      "Epoch[10/70], Step[19500], Loss: 0.0125, lr: 0.000010\n",
      "Epoch[10/70], Step[19600], Loss: 0.0026, lr: 0.000010\n",
      "Epoch[10/70], Step[19700], Loss: 0.0011, lr: 0.000010\n",
      "Epoch[10/70], Step[19800], Loss: 0.0666, lr: 0.000010\n",
      "Epoch[10/70], Step[19900], Loss: 0.0698, lr: 0.000010\n",
      "Epoch[11/70], Step[20000], Loss: 0.0383, lr: 0.000010\n",
      "Epoch[11/70], Step[20100], Loss: 0.0033, lr: 0.000010\n",
      "Epoch[11/70], Step[20200], Loss: 0.0167, lr: 0.000009\n",
      "Epoch[11/70], Step[20300], Loss: 0.0122, lr: 0.000009\n",
      "Epoch[11/70], Step[20400], Loss: 0.0020, lr: 0.000009\n",
      "Epoch[11/70], Step[20500], Loss: 0.0011, lr: 0.000009\n",
      "Epoch[11/70], Step[20600], Loss: 0.0156, lr: 0.000009\n",
      "Epoch[11/70], Step[20700], Loss: 0.0298, lr: 0.000009\n",
      "Epoch[11/70], Step[20800], Loss: 0.0200, lr: 0.000009\n",
      "Epoch[11/70], Step[20900], Loss: 0.0117, lr: 0.000009\n",
      "Epoch[11/70], Step[21000], Loss: 0.0079, lr: 0.000009\n",
      "Epoch[11/70], Step[21100], Loss: 0.0015, lr: 0.000009\n",
      "Epoch[11/70], Step[21200], Loss: 0.0122, lr: 0.000009\n",
      "Epoch[11/70], Step[21300], Loss: 0.0016, lr: 0.000009\n",
      "Epoch[11/70], Step[21400], Loss: 0.0170, lr: 0.000009\n",
      "Epoch[11/70], Step[21500], Loss: 0.0150, lr: 0.000009\n",
      "Epoch[11/70], Step[21600], Loss: 0.0015, lr: 0.000009\n",
      "Epoch[11/70], Step[21700], Loss: 0.0899, lr: 0.000009\n",
      "Epoch[11/70], Step[21800], Loss: 0.0063, lr: 0.000009\n",
      "Epoch[11/70], Step[21900], Loss: 0.0212, lr: 0.000009\n",
      "Epoch[12/70], Step[22000], Loss: 0.0076, lr: 0.000009\n",
      "Epoch[12/70], Step[22100], Loss: 0.0188, lr: 0.000009\n",
      "Epoch[12/70], Step[22200], Loss: 0.0016, lr: 0.000009\n",
      "Epoch[12/70], Step[22300], Loss: 0.0068, lr: 0.000009\n",
      "Epoch[12/70], Step[22400], Loss: 0.0072, lr: 0.000009\n",
      "Epoch[12/70], Step[22500], Loss: 0.0007, lr: 0.000009\n",
      "Epoch[12/70], Step[22600], Loss: 0.0592, lr: 0.000009\n",
      "Epoch[12/70], Step[22700], Loss: 0.0535, lr: 0.000009\n",
      "Epoch[12/70], Step[22800], Loss: 0.0214, lr: 0.000009\n",
      "Epoch[12/70], Step[22900], Loss: 0.0135, lr: 0.000009\n",
      "Epoch[12/70], Step[23000], Loss: 0.0238, lr: 0.000009\n",
      "Epoch[12/70], Step[23100], Loss: 0.0594, lr: 0.000009\n",
      "Epoch[12/70], Step[23200], Loss: 0.0463, lr: 0.000009\n",
      "Epoch[12/70], Step[23300], Loss: 0.0028, lr: 0.000009\n",
      "Epoch[12/70], Step[23400], Loss: 0.0031, lr: 0.000009\n",
      "Epoch[12/70], Step[23500], Loss: 0.0011, lr: 0.000009\n",
      "Epoch[12/70], Step[23600], Loss: 0.0029, lr: 0.000009\n",
      "Epoch[12/70], Step[23700], Loss: 0.0008, lr: 0.000009\n",
      "Epoch[12/70], Step[23800], Loss: 0.0143, lr: 0.000009\n",
      "Epoch[12/70], Step[23900], Loss: 0.0683, lr: 0.000009\n",
      "Epoch[13/70], Step[24000], Loss: 0.0036, lr: 0.000009\n",
      "Epoch[13/70], Step[24100], Loss: 0.0285, lr: 0.000009\n",
      "Epoch[13/70], Step[24200], Loss: 0.0182, lr: 0.000009\n",
      "Epoch[13/70], Step[24300], Loss: 0.0115, lr: 0.000009\n",
      "Epoch[13/70], Step[24400], Loss: 0.0223, lr: 0.000009\n",
      "Epoch[13/70], Step[24500], Loss: 0.0089, lr: 0.000009\n",
      "Epoch[13/70], Step[24600], Loss: 0.0915, lr: 0.000009\n",
      "Epoch[13/70], Step[24700], Loss: 0.0689, lr: 0.000009\n",
      "Epoch[13/70], Step[24800], Loss: 0.0614, lr: 0.000009\n",
      "Epoch[13/70], Step[24900], Loss: 0.0066, lr: 0.000009\n",
      "Epoch[13/70], Step[25000], Loss: 0.0021, lr: 0.000009\n",
      "Epoch[13/70], Step[25100], Loss: 0.0020, lr: 0.000009\n",
      "Epoch[13/70], Step[25200], Loss: 0.0328, lr: 0.000009\n",
      "Epoch[13/70], Step[25300], Loss: 0.0061, lr: 0.000009\n",
      "Epoch[13/70], Step[25400], Loss: 0.0046, lr: 0.000009\n",
      "Epoch[13/70], Step[25500], Loss: 0.0051, lr: 0.000009\n",
      "Epoch[13/70], Step[25600], Loss: 0.0170, lr: 0.000009\n",
      "Epoch[13/70], Step[25700], Loss: 0.0504, lr: 0.000009\n",
      "Epoch[13/70], Step[25800], Loss: 0.0906, lr: 0.000009\n",
      "Epoch[13/70], Step[25900], Loss: 0.0237, lr: 0.000009\n",
      "Epoch[14/70], Step[26000], Loss: 0.0500, lr: 0.000009\n",
      "Epoch[14/70], Step[26100], Loss: 0.0134, lr: 0.000009\n",
      "Epoch[14/70], Step[26200], Loss: 0.0213, lr: 0.000009\n",
      "Epoch[14/70], Step[26300], Loss: 0.0022, lr: 0.000009\n",
      "Epoch[14/70], Step[26400], Loss: 0.0021, lr: 0.000009\n",
      "Epoch[14/70], Step[26500], Loss: 0.0107, lr: 0.000009\n",
      "Epoch[14/70], Step[26600], Loss: 0.0716, lr: 0.000009\n",
      "Epoch[14/70], Step[26700], Loss: 0.0015, lr: 0.000009\n",
      "Epoch[14/70], Step[26800], Loss: 0.0401, lr: 0.000009\n",
      "Epoch[14/70], Step[26900], Loss: 0.0175, lr: 0.000009\n",
      "Epoch[14/70], Step[27000], Loss: 0.0254, lr: 0.000009\n",
      "Epoch[14/70], Step[27100], Loss: 0.0058, lr: 0.000009\n",
      "Epoch[14/70], Step[27200], Loss: 0.0348, lr: 0.000009\n",
      "Epoch[14/70], Step[27300], Loss: 0.0016, lr: 0.000009\n",
      "Epoch[14/70], Step[27400], Loss: 0.0016, lr: 0.000009\n",
      "Epoch[14/70], Step[27500], Loss: 0.0155, lr: 0.000009\n",
      "Epoch[14/70], Step[27600], Loss: 0.0029, lr: 0.000009\n",
      "Epoch[14/70], Step[27700], Loss: 0.0252, lr: 0.000009\n",
      "Epoch[14/70], Step[27800], Loss: 0.0535, lr: 0.000009\n",
      "Epoch[14/70], Step[27900], Loss: 0.0287, lr: 0.000009\n",
      "Epoch[15/70], Step[28000], Loss: 0.0155, lr: 0.000009\n",
      "Epoch[15/70], Step[28100], Loss: 0.0097, lr: 0.000009\n",
      "Epoch[15/70], Step[28200], Loss: 0.0155, lr: 0.000009\n",
      "Epoch[15/70], Step[28300], Loss: 0.0108, lr: 0.000009\n",
      "Epoch[15/70], Step[28400], Loss: 0.0036, lr: 0.000009\n",
      "Epoch[15/70], Step[28500], Loss: 0.0030, lr: 0.000009\n",
      "Epoch[15/70], Step[28600], Loss: 0.0012, lr: 0.000009\n",
      "Epoch[15/70], Step[28700], Loss: 0.0660, lr: 0.000009\n",
      "Epoch[15/70], Step[28800], Loss: 0.1693, lr: 0.000009\n",
      "Epoch[15/70], Step[28900], Loss: 0.0032, lr: 0.000009\n",
      "Epoch[15/70], Step[29000], Loss: 0.0264, lr: 0.000009\n",
      "Epoch[15/70], Step[29100], Loss: 0.0412, lr: 0.000009\n",
      "Epoch[15/70], Step[29200], Loss: 0.0469, lr: 0.000009\n",
      "Epoch[15/70], Step[29300], Loss: 0.0010, lr: 0.000009\n",
      "Epoch[15/70], Step[29400], Loss: 0.0016, lr: 0.000009\n",
      "Epoch[15/70], Step[29500], Loss: 0.0146, lr: 0.000009\n",
      "Epoch[15/70], Step[29600], Loss: 0.0054, lr: 0.000009\n",
      "Epoch[15/70], Step[29700], Loss: 0.0909, lr: 0.000009\n",
      "Epoch[15/70], Step[29800], Loss: 0.0650, lr: 0.000009\n",
      "Epoch[15/70], Step[29900], Loss: 0.0596, lr: 0.000009\n",
      "Epoch[16/70], Step[30000], Loss: 0.0549, lr: 0.000009\n",
      "Epoch[16/70], Step[30100], Loss: 0.0169, lr: 0.000009\n",
      "Epoch[16/70], Step[30200], Loss: 0.0005, lr: 0.000009\n",
      "Epoch[16/70], Step[30300], Loss: 0.0057, lr: 0.000009\n",
      "Epoch[16/70], Step[30400], Loss: 0.0215, lr: 0.000009\n",
      "Epoch[16/70], Step[30500], Loss: 0.0167, lr: 0.000009\n",
      "Epoch[16/70], Step[30600], Loss: 0.0555, lr: 0.000009\n",
      "Epoch[16/70], Step[30700], Loss: 0.0167, lr: 0.000009\n",
      "Epoch[16/70], Step[30800], Loss: 0.1315, lr: 0.000009\n",
      "Epoch[16/70], Step[30900], Loss: 0.0033, lr: 0.000009\n",
      "Epoch[16/70], Step[31000], Loss: 0.0219, lr: 0.000009\n",
      "Epoch[16/70], Step[31100], Loss: 0.0023, lr: 0.000009\n",
      "Epoch[16/70], Step[31200], Loss: 0.0119, lr: 0.000009\n",
      "Epoch[16/70], Step[31300], Loss: 0.0013, lr: 0.000009\n",
      "Epoch[16/70], Step[31400], Loss: 0.0070, lr: 0.000009\n",
      "Epoch[16/70], Step[31500], Loss: 0.0028, lr: 0.000009\n",
      "Epoch[16/70], Step[31600], Loss: 0.0024, lr: 0.000009\n",
      "Epoch[16/70], Step[31700], Loss: 0.0812, lr: 0.000009\n",
      "Epoch[16/70], Step[31800], Loss: 0.0272, lr: 0.000009\n",
      "Epoch[16/70], Step[31900], Loss: 0.0570, lr: 0.000009\n",
      "Epoch[17/70], Step[32000], Loss: 0.0350, lr: 0.000009\n",
      "Epoch[17/70], Step[32100], Loss: 0.0137, lr: 0.000009\n",
      "Epoch[17/70], Step[32200], Loss: 0.0134, lr: 0.000009\n",
      "Epoch[17/70], Step[32300], Loss: 0.0008, lr: 0.000009\n",
      "Epoch[17/70], Step[32400], Loss: 0.0013, lr: 0.000009\n",
      "Epoch[17/70], Step[32500], Loss: 0.0064, lr: 0.000009\n",
      "Epoch[17/70], Step[32600], Loss: 0.0035, lr: 0.000009\n",
      "Epoch[17/70], Step[32700], Loss: 0.0507, lr: 0.000009\n",
      "Epoch[17/70], Step[32800], Loss: 0.1009, lr: 0.000009\n",
      "Epoch[17/70], Step[32900], Loss: 0.0016, lr: 0.000009\n",
      "Epoch[17/70], Step[33000], Loss: 0.0075, lr: 0.000009\n",
      "Epoch[17/70], Step[33100], Loss: 0.0005, lr: 0.000009\n",
      "Epoch[17/70], Step[33200], Loss: 0.0049, lr: 0.000009\n",
      "Epoch[17/70], Step[33300], Loss: 0.0025, lr: 0.000009\n",
      "Epoch[17/70], Step[33400], Loss: 0.0136, lr: 0.000009\n",
      "Epoch[17/70], Step[33500], Loss: 0.0057, lr: 0.000009\n",
      "Epoch[17/70], Step[33600], Loss: 0.0107, lr: 0.000009\n",
      "Epoch[17/70], Step[33700], Loss: 0.0595, lr: 0.000009\n",
      "Epoch[17/70], Step[33800], Loss: 0.0087, lr: 0.000009\n",
      "Epoch[17/70], Step[33900], Loss: 0.0005, lr: 0.000009\n",
      "Epoch[18/70], Step[34000], Loss: 0.0773, lr: 0.000009\n",
      "Epoch[18/70], Step[34100], Loss: 0.0003, lr: 0.000009\n",
      "Epoch[18/70], Step[34200], Loss: 0.0248, lr: 0.000009\n",
      "Epoch[18/70], Step[34300], Loss: 0.0179, lr: 0.000009\n",
      "Epoch[18/70], Step[34400], Loss: 0.0052, lr: 0.000009\n",
      "Epoch[18/70], Step[34500], Loss: 0.0017, lr: 0.000009\n",
      "Epoch[18/70], Step[34600], Loss: 0.0764, lr: 0.000009\n",
      "Epoch[18/70], Step[34700], Loss: 0.0164, lr: 0.000009\n",
      "Epoch[18/70], Step[34800], Loss: 0.0104, lr: 0.000009\n",
      "Epoch[18/70], Step[34900], Loss: 0.0019, lr: 0.000009\n",
      "Epoch[18/70], Step[35000], Loss: 0.0069, lr: 0.000009\n",
      "Epoch[18/70], Step[35100], Loss: 0.0620, lr: 0.000009\n",
      "Epoch[18/70], Step[35200], Loss: 0.0219, lr: 0.000009\n",
      "Epoch[18/70], Step[35300], Loss: 0.0098, lr: 0.000009\n",
      "Epoch[18/70], Step[35400], Loss: 0.0174, lr: 0.000009\n",
      "Epoch[18/70], Step[35500], Loss: 0.0072, lr: 0.000009\n",
      "Epoch[18/70], Step[35600], Loss: 0.0056, lr: 0.000008\n",
      "Epoch[18/70], Step[35700], Loss: 0.0226, lr: 0.000008\n",
      "Epoch[18/70], Step[35800], Loss: 0.0121, lr: 0.000008\n",
      "Epoch[18/70], Step[35900], Loss: 0.0055, lr: 0.000008\n",
      "Epoch[19/70], Step[36000], Loss: 0.0728, lr: 0.000008\n",
      "Epoch[19/70], Step[36100], Loss: 0.0252, lr: 0.000008\n",
      "Epoch[19/70], Step[36200], Loss: 0.0018, lr: 0.000008\n",
      "Epoch[19/70], Step[36300], Loss: 0.0095, lr: 0.000008\n",
      "Epoch[19/70], Step[36400], Loss: 0.0208, lr: 0.000008\n",
      "Epoch[19/70], Step[36500], Loss: 0.0136, lr: 0.000008\n",
      "Epoch[19/70], Step[36600], Loss: 0.0031, lr: 0.000008\n",
      "Epoch[19/70], Step[36700], Loss: 0.0017, lr: 0.000008\n",
      "Epoch[19/70], Step[36800], Loss: 0.0495, lr: 0.000008\n",
      "Epoch[19/70], Step[36900], Loss: 0.0077, lr: 0.000008\n",
      "Epoch[19/70], Step[37000], Loss: 0.0211, lr: 0.000008\n",
      "Epoch[19/70], Step[37100], Loss: 0.0561, lr: 0.000008\n",
      "Epoch[19/70], Step[37200], Loss: 0.0402, lr: 0.000008\n",
      "Epoch[19/70], Step[37300], Loss: 0.0027, lr: 0.000008\n",
      "Epoch[19/70], Step[37400], Loss: 0.0012, lr: 0.000008\n",
      "Epoch[19/70], Step[37500], Loss: 0.0156, lr: 0.000008\n",
      "Epoch[19/70], Step[37600], Loss: 0.0006, lr: 0.000008\n",
      "Epoch[19/70], Step[37700], Loss: 0.0989, lr: 0.000008\n",
      "Epoch[19/70], Step[37800], Loss: 0.0703, lr: 0.000008\n",
      "Epoch[19/70], Step[37900], Loss: 0.0032, lr: 0.000008\n",
      "Epoch[20/70], Step[38000], Loss: 0.0622, lr: 0.000008\n",
      "Epoch[20/70], Step[38100], Loss: 0.0094, lr: 0.000008\n",
      "Epoch[20/70], Step[38200], Loss: 0.0021, lr: 0.000008\n",
      "Epoch[20/70], Step[38300], Loss: 0.0023, lr: 0.000008\n",
      "Epoch[20/70], Step[38400], Loss: 0.0023, lr: 0.000008\n",
      "Epoch[20/70], Step[38500], Loss: 0.0034, lr: 0.000008\n",
      "Epoch[20/70], Step[38600], Loss: 0.0596, lr: 0.000008\n",
      "Epoch[20/70], Step[38700], Loss: 0.0201, lr: 0.000008\n",
      "Epoch[20/70], Step[38800], Loss: 0.1577, lr: 0.000008\n",
      "Epoch[20/70], Step[38900], Loss: 0.0027, lr: 0.000008\n",
      "Epoch[20/70], Step[39000], Loss: 0.0059, lr: 0.000008\n",
      "Epoch[20/70], Step[39100], Loss: 0.0402, lr: 0.000008\n",
      "Epoch[20/70], Step[39200], Loss: 0.0370, lr: 0.000008\n",
      "Epoch[20/70], Step[39300], Loss: 0.0008, lr: 0.000008\n",
      "Epoch[20/70], Step[39400], Loss: 0.0060, lr: 0.000008\n",
      "Epoch[20/70], Step[39500], Loss: 0.0106, lr: 0.000008\n",
      "Epoch[20/70], Step[39600], Loss: 0.0178, lr: 0.000008\n",
      "Epoch[20/70], Step[39700], Loss: 0.0550, lr: 0.000008\n",
      "Epoch[20/70], Step[39800], Loss: 0.0065, lr: 0.000008\n",
      "Epoch[20/70], Step[39900], Loss: 0.0446, lr: 0.000008\n",
      "Epoch[21/70], Step[40000], Loss: 0.0174, lr: 0.000008\n",
      "Epoch[21/70], Step[40100], Loss: 0.0130, lr: 0.000008\n",
      "Epoch[21/70], Step[40200], Loss: 0.0249, lr: 0.000008\n",
      "Epoch[21/70], Step[40300], Loss: 0.0040, lr: 0.000008\n",
      "Epoch[21/70], Step[40400], Loss: 0.0081, lr: 0.000008\n",
      "Epoch[21/70], Step[40500], Loss: 0.0010, lr: 0.000008\n",
      "Epoch[21/70], Step[40600], Loss: 0.0758, lr: 0.000008\n",
      "Epoch[21/70], Step[40700], Loss: 0.0831, lr: 0.000008\n",
      "Epoch[21/70], Step[40800], Loss: 0.0214, lr: 0.000008\n",
      "Epoch[21/70], Step[40900], Loss: 0.0042, lr: 0.000008\n",
      "Epoch[21/70], Step[41000], Loss: 0.0214, lr: 0.000008\n",
      "Epoch[21/70], Step[41100], Loss: 0.0277, lr: 0.000008\n",
      "Epoch[21/70], Step[41200], Loss: 0.0160, lr: 0.000008\n",
      "Epoch[21/70], Step[41300], Loss: 0.0010, lr: 0.000008\n",
      "Epoch[21/70], Step[41400], Loss: 0.0060, lr: 0.000008\n",
      "Epoch[21/70], Step[41500], Loss: 0.0089, lr: 0.000008\n",
      "Epoch[21/70], Step[41600], Loss: 0.0025, lr: 0.000008\n",
      "Epoch[21/70], Step[41700], Loss: 0.0286, lr: 0.000008\n",
      "Epoch[21/70], Step[41800], Loss: 0.0062, lr: 0.000008\n",
      "Epoch[21/70], Step[41900], Loss: 0.0048, lr: 0.000008\n",
      "Epoch[22/70], Step[42000], Loss: 0.0763, lr: 0.000008\n",
      "Epoch[22/70], Step[42100], Loss: 0.0037, lr: 0.000008\n",
      "Epoch[22/70], Step[42200], Loss: 0.0167, lr: 0.000008\n",
      "Epoch[22/70], Step[42300], Loss: 0.0075, lr: 0.000008\n",
      "Epoch[22/70], Step[42400], Loss: 0.0024, lr: 0.000008\n",
      "Epoch[22/70], Step[42500], Loss: 0.0151, lr: 0.000008\n",
      "Epoch[22/70], Step[42600], Loss: 0.0551, lr: 0.000008\n",
      "Epoch[22/70], Step[42700], Loss: 0.0392, lr: 0.000008\n",
      "Epoch[22/70], Step[42800], Loss: 0.0789, lr: 0.000008\n",
      "Epoch[22/70], Step[42900], Loss: 0.0032, lr: 0.000008\n",
      "Epoch[22/70], Step[43000], Loss: 0.0206, lr: 0.000008\n",
      "Epoch[22/70], Step[43100], Loss: 0.0218, lr: 0.000008\n",
      "Epoch[22/70], Step[43200], Loss: 0.0252, lr: 0.000008\n",
      "Epoch[22/70], Step[43300], Loss: 0.0046, lr: 0.000008\n",
      "Epoch[22/70], Step[43400], Loss: 0.0017, lr: 0.000008\n",
      "Epoch[22/70], Step[43500], Loss: 0.0012, lr: 0.000008\n",
      "Epoch[22/70], Step[43600], Loss: 0.0019, lr: 0.000008\n",
      "Epoch[22/70], Step[43700], Loss: 0.0239, lr: 0.000008\n",
      "Epoch[22/70], Step[43800], Loss: 0.0226, lr: 0.000008\n",
      "Epoch[22/70], Step[43900], Loss: 0.0061, lr: 0.000008\n",
      "Epoch[23/70], Step[44000], Loss: 0.0804, lr: 0.000008\n",
      "Epoch[23/70], Step[44100], Loss: 0.0025, lr: 0.000008\n",
      "Epoch[23/70], Step[44200], Loss: 0.0064, lr: 0.000008\n",
      "Epoch[23/70], Step[44300], Loss: 0.0083, lr: 0.000008\n",
      "Epoch[23/70], Step[44400], Loss: 0.0023, lr: 0.000008\n",
      "Epoch[23/70], Step[44500], Loss: 0.0026, lr: 0.000008\n",
      "Epoch[23/70], Step[44600], Loss: 0.0600, lr: 0.000008\n",
      "Epoch[23/70], Step[44700], Loss: 0.0058, lr: 0.000008\n",
      "Epoch[23/70], Step[44800], Loss: 0.0480, lr: 0.000008\n",
      "Epoch[23/70], Step[44900], Loss: 0.0009, lr: 0.000008\n",
      "Epoch[23/70], Step[45000], Loss: 0.0027, lr: 0.000008\n",
      "Epoch[23/70], Step[45100], Loss: 0.0116, lr: 0.000008\n",
      "Epoch[23/70], Step[45200], Loss: 0.0291, lr: 0.000008\n",
      "Epoch[23/70], Step[45300], Loss: 0.0038, lr: 0.000008\n",
      "Epoch[23/70], Step[45400], Loss: 0.0006, lr: 0.000008\n",
      "Epoch[23/70], Step[45500], Loss: 0.0003, lr: 0.000008\n",
      "Epoch[23/70], Step[45600], Loss: 0.0097, lr: 0.000008\n",
      "Epoch[23/70], Step[45700], Loss: 0.0425, lr: 0.000008\n",
      "Epoch[23/70], Step[45800], Loss: 0.0478, lr: 0.000008\n",
      "Epoch[23/70], Step[45900], Loss: 0.0438, lr: 0.000008\n",
      "Epoch[24/70], Step[46000], Loss: 0.0355, lr: 0.000008\n",
      "Epoch[24/70], Step[46100], Loss: 0.0012, lr: 0.000008\n",
      "Epoch[24/70], Step[46200], Loss: 0.0200, lr: 0.000008\n",
      "Epoch[24/70], Step[46300], Loss: 0.0036, lr: 0.000008\n",
      "Epoch[24/70], Step[46400], Loss: 0.0209, lr: 0.000008\n",
      "Epoch[24/70], Step[46500], Loss: 0.0055, lr: 0.000008\n",
      "Epoch[24/70], Step[46600], Loss: 0.0164, lr: 0.000008\n",
      "Epoch[24/70], Step[46700], Loss: 0.0497, lr: 0.000008\n",
      "Epoch[24/70], Step[46800], Loss: 0.1307, lr: 0.000007\n",
      "Epoch[24/70], Step[46900], Loss: 0.0063, lr: 0.000007\n",
      "Epoch[24/70], Step[47000], Loss: 0.0009, lr: 0.000007\n",
      "Epoch[24/70], Step[47100], Loss: 0.0247, lr: 0.000007\n",
      "Epoch[24/70], Step[47200], Loss: 0.0348, lr: 0.000007\n",
      "Epoch[24/70], Step[47300], Loss: 0.0110, lr: 0.000007\n",
      "Epoch[24/70], Step[47400], Loss: 0.0111, lr: 0.000007\n",
      "Epoch[24/70], Step[47500], Loss: 0.0015, lr: 0.000007\n",
      "Epoch[24/70], Step[47600], Loss: 0.0029, lr: 0.000007\n",
      "Epoch[24/70], Step[47700], Loss: 0.0046, lr: 0.000007\n",
      "Epoch[24/70], Step[47800], Loss: 0.0242, lr: 0.000007\n",
      "Epoch[24/70], Step[47900], Loss: 0.0093, lr: 0.000007\n",
      "Epoch[25/70], Step[48000], Loss: 0.0136, lr: 0.000007\n",
      "Epoch[25/70], Step[48100], Loss: 0.0103, lr: 0.000007\n",
      "Epoch[25/70], Step[48200], Loss: 0.0207, lr: 0.000007\n",
      "Epoch[25/70], Step[48300], Loss: 0.0193, lr: 0.000007\n",
      "Epoch[25/70], Step[48400], Loss: 0.0164, lr: 0.000007\n",
      "Epoch[25/70], Step[48500], Loss: 0.0162, lr: 0.000007\n",
      "Epoch[25/70], Step[48600], Loss: 0.0053, lr: 0.000007\n",
      "Epoch[25/70], Step[48700], Loss: 0.0125, lr: 0.000007\n",
      "Epoch[25/70], Step[48800], Loss: 0.0805, lr: 0.000007\n",
      "Epoch[25/70], Step[48900], Loss: 0.0205, lr: 0.000007\n",
      "Epoch[25/70], Step[49000], Loss: 0.0012, lr: 0.000007\n",
      "Epoch[25/70], Step[49100], Loss: 0.0662, lr: 0.000007\n",
      "Epoch[25/70], Step[49200], Loss: 0.0056, lr: 0.000007\n",
      "Epoch[25/70], Step[49300], Loss: 0.0119, lr: 0.000007\n",
      "Epoch[25/70], Step[49400], Loss: 0.0014, lr: 0.000007\n",
      "Epoch[25/70], Step[49500], Loss: 0.0016, lr: 0.000007\n",
      "Epoch[25/70], Step[49600], Loss: 0.0011, lr: 0.000007\n",
      "Epoch[25/70], Step[49700], Loss: 0.0127, lr: 0.000007\n",
      "Epoch[25/70], Step[49800], Loss: 0.0693, lr: 0.000007\n",
      "Epoch[25/70], Step[49900], Loss: 0.0017, lr: 0.000007\n",
      "Epoch[26/70], Step[50000], Loss: 0.0793, lr: 0.000007\n",
      "Epoch[26/70], Step[50100], Loss: 0.0098, lr: 0.000007\n",
      "Epoch[26/70], Step[50200], Loss: 0.0007, lr: 0.000007\n",
      "Epoch[26/70], Step[50300], Loss: 0.0119, lr: 0.000007\n",
      "Epoch[26/70], Step[50400], Loss: 0.0034, lr: 0.000007\n",
      "Epoch[26/70], Step[50500], Loss: 0.0194, lr: 0.000007\n",
      "Epoch[26/70], Step[50600], Loss: 0.0235, lr: 0.000007\n",
      "Epoch[26/70], Step[50700], Loss: 0.0722, lr: 0.000007\n",
      "Epoch[26/70], Step[50800], Loss: 0.0751, lr: 0.000007\n",
      "Epoch[26/70], Step[50900], Loss: 0.0051, lr: 0.000007\n",
      "Epoch[26/70], Step[51000], Loss: 0.0025, lr: 0.000007\n",
      "Epoch[26/70], Step[51100], Loss: 0.0125, lr: 0.000007\n",
      "Epoch[26/70], Step[51200], Loss: 0.0279, lr: 0.000007\n",
      "Epoch[26/70], Step[51300], Loss: 0.0031, lr: 0.000007\n",
      "Epoch[26/70], Step[51400], Loss: 0.0077, lr: 0.000007\n",
      "Epoch[26/70], Step[51500], Loss: 0.0164, lr: 0.000007\n",
      "Epoch[26/70], Step[51600], Loss: 0.0016, lr: 0.000007\n",
      "Epoch[26/70], Step[51700], Loss: 0.0599, lr: 0.000007\n",
      "Epoch[26/70], Step[51800], Loss: 0.0189, lr: 0.000007\n",
      "Epoch[26/70], Step[51900], Loss: 0.0516, lr: 0.000007\n",
      "Epoch[27/70], Step[52000], Loss: 0.0393, lr: 0.000007\n",
      "Epoch[27/70], Step[52100], Loss: 0.0107, lr: 0.000007\n",
      "Epoch[27/70], Step[52200], Loss: 0.0251, lr: 0.000007\n",
      "Epoch[27/70], Step[52300], Loss: 0.0034, lr: 0.000007\n",
      "Epoch[27/70], Step[52400], Loss: 0.0123, lr: 0.000007\n",
      "Epoch[27/70], Step[52500], Loss: 0.0047, lr: 0.000007\n",
      "Epoch[27/70], Step[52600], Loss: 0.0056, lr: 0.000007\n",
      "Epoch[27/70], Step[52700], Loss: 0.0259, lr: 0.000007\n",
      "Epoch[27/70], Step[52800], Loss: 0.0208, lr: 0.000007\n",
      "Epoch[27/70], Step[52900], Loss: 0.0095, lr: 0.000007\n",
      "Epoch[27/70], Step[53000], Loss: 0.0012, lr: 0.000007\n",
      "Epoch[27/70], Step[53100], Loss: 0.0605, lr: 0.000007\n",
      "Epoch[27/70], Step[53200], Loss: 0.0284, lr: 0.000007\n",
      "Epoch[27/70], Step[53300], Loss: 0.0142, lr: 0.000007\n",
      "Epoch[27/70], Step[53400], Loss: 0.0169, lr: 0.000007\n",
      "Epoch[27/70], Step[53500], Loss: 0.0082, lr: 0.000007\n",
      "Epoch[27/70], Step[53600], Loss: 0.0181, lr: 0.000007\n",
      "Epoch[27/70], Step[53700], Loss: 0.0497, lr: 0.000007\n",
      "Epoch[27/70], Step[53800], Loss: 0.0232, lr: 0.000007\n",
      "Epoch[27/70], Step[53900], Loss: 0.0035, lr: 0.000007\n",
      "Epoch[28/70], Step[54000], Loss: 0.0242, lr: 0.000007\n",
      "Epoch[28/70], Step[54100], Loss: 0.0190, lr: 0.000007\n",
      "Epoch[28/70], Step[54200], Loss: 0.0022, lr: 0.000007\n",
      "Epoch[28/70], Step[54300], Loss: 0.0052, lr: 0.000007\n",
      "Epoch[28/70], Step[54400], Loss: 0.0026, lr: 0.000007\n",
      "Epoch[28/70], Step[54500], Loss: 0.0015, lr: 0.000007\n",
      "Epoch[28/70], Step[54600], Loss: 0.0300, lr: 0.000007\n",
      "Epoch[28/70], Step[54700], Loss: 0.0256, lr: 0.000007\n",
      "Epoch[28/70], Step[54800], Loss: 0.0756, lr: 0.000007\n",
      "Epoch[28/70], Step[54900], Loss: 0.0020, lr: 0.000007\n",
      "Epoch[28/70], Step[55000], Loss: 0.0056, lr: 0.000007\n",
      "Epoch[28/70], Step[55100], Loss: 0.0201, lr: 0.000007\n",
      "Epoch[28/70], Step[55200], Loss: 0.0575, lr: 0.000007\n",
      "Epoch[28/70], Step[55300], Loss: 0.0009, lr: 0.000007\n",
      "Epoch[28/70], Step[55400], Loss: 0.0118, lr: 0.000007\n",
      "Epoch[28/70], Step[55500], Loss: 0.0126, lr: 0.000007\n",
      "Epoch[28/70], Step[55600], Loss: 0.0019, lr: 0.000007\n",
      "Epoch[28/70], Step[55700], Loss: 0.1045, lr: 0.000007\n",
      "Epoch[28/70], Step[55800], Loss: 0.0571, lr: 0.000007\n",
      "Epoch[28/70], Step[55900], Loss: 0.0029, lr: 0.000007\n",
      "Epoch[29/70], Step[56000], Loss: 0.0143, lr: 0.000007\n",
      "Epoch[29/70], Step[56100], Loss: 0.0114, lr: 0.000007\n",
      "Epoch[29/70], Step[56200], Loss: 0.0004, lr: 0.000007\n",
      "Epoch[29/70], Step[56300], Loss: 0.0006, lr: 0.000007\n",
      "Epoch[29/70], Step[56400], Loss: 0.0034, lr: 0.000007\n",
      "Epoch[29/70], Step[56500], Loss: 0.0087, lr: 0.000006\n",
      "Epoch[29/70], Step[56600], Loss: 0.0187, lr: 0.000006\n",
      "Epoch[29/70], Step[56700], Loss: 0.0231, lr: 0.000006\n",
      "Epoch[29/70], Step[56800], Loss: 0.1162, lr: 0.000006\n",
      "Epoch[29/70], Step[56900], Loss: 0.0123, lr: 0.000006\n",
      "Epoch[29/70], Step[57000], Loss: 0.0263, lr: 0.000006\n",
      "Epoch[29/70], Step[57100], Loss: 0.0150, lr: 0.000006\n",
      "Epoch[29/70], Step[57200], Loss: 0.0046, lr: 0.000006\n",
      "Epoch[29/70], Step[57300], Loss: 0.0021, lr: 0.000006\n",
      "Epoch[29/70], Step[57400], Loss: 0.0171, lr: 0.000006\n",
      "Epoch[29/70], Step[57500], Loss: 0.0152, lr: 0.000006\n",
      "Epoch[29/70], Step[57600], Loss: 0.0078, lr: 0.000006\n",
      "Epoch[29/70], Step[57700], Loss: 0.0669, lr: 0.000006\n",
      "Epoch[29/70], Step[57800], Loss: 0.0749, lr: 0.000006\n",
      "Epoch[29/70], Step[57900], Loss: 0.0695, lr: 0.000006\n",
      "Epoch[30/70], Step[58000], Loss: 0.0078, lr: 0.000006\n",
      "Epoch[30/70], Step[58100], Loss: 0.0070, lr: 0.000006\n",
      "Epoch[30/70], Step[58200], Loss: 0.0256, lr: 0.000006\n",
      "Epoch[30/70], Step[58300], Loss: 0.0054, lr: 0.000006\n",
      "Epoch[30/70], Step[58400], Loss: 0.0004, lr: 0.000006\n",
      "Epoch[30/70], Step[58500], Loss: 0.0023, lr: 0.000006\n",
      "Epoch[30/70], Step[58600], Loss: 0.0835, lr: 0.000006\n",
      "Epoch[30/70], Step[58700], Loss: 0.0607, lr: 0.000006\n",
      "Epoch[30/70], Step[58800], Loss: 0.0389, lr: 0.000006\n",
      "Epoch[30/70], Step[58900], Loss: 0.0036, lr: 0.000006\n",
      "Epoch[30/70], Step[59000], Loss: 0.0048, lr: 0.000006\n",
      "Epoch[30/70], Step[59100], Loss: 0.0018, lr: 0.000006\n",
      "Epoch[30/70], Step[59200], Loss: 0.0607, lr: 0.000006\n",
      "Epoch[30/70], Step[59300], Loss: 0.0052, lr: 0.000006\n",
      "Epoch[30/70], Step[59400], Loss: 0.0133, lr: 0.000006\n",
      "Epoch[30/70], Step[59500], Loss: 0.0021, lr: 0.000006\n",
      "Epoch[30/70], Step[59600], Loss: 0.0009, lr: 0.000006\n",
      "Epoch[30/70], Step[59700], Loss: 0.0808, lr: 0.000006\n",
      "Epoch[30/70], Step[59800], Loss: 0.0783, lr: 0.000006\n",
      "Epoch[30/70], Step[59900], Loss: 0.0026, lr: 0.000006\n",
      "Epoch[31/70], Step[60000], Loss: 0.0020, lr: 0.000006\n",
      "Epoch[31/70], Step[60100], Loss: 0.0105, lr: 0.000006\n",
      "Epoch[31/70], Step[60200], Loss: 0.0016, lr: 0.000006\n",
      "Epoch[31/70], Step[60300], Loss: 0.0100, lr: 0.000006\n",
      "Epoch[31/70], Step[60400], Loss: 0.0032, lr: 0.000006\n",
      "Epoch[31/70], Step[60500], Loss: 0.0042, lr: 0.000006\n",
      "Epoch[31/70], Step[60600], Loss: 0.0421, lr: 0.000006\n",
      "Epoch[31/70], Step[60700], Loss: 0.0611, lr: 0.000006\n",
      "Epoch[31/70], Step[60800], Loss: 0.0966, lr: 0.000006\n",
      "Epoch[31/70], Step[60900], Loss: 0.0208, lr: 0.000006\n",
      "Epoch[31/70], Step[61000], Loss: 0.0100, lr: 0.000006\n",
      "Epoch[31/70], Step[61100], Loss: 0.0386, lr: 0.000006\n",
      "Epoch[31/70], Step[61200], Loss: 0.0452, lr: 0.000006\n",
      "Epoch[31/70], Step[61300], Loss: 0.0023, lr: 0.000006\n",
      "Epoch[31/70], Step[61400], Loss: 0.0164, lr: 0.000006\n",
      "Epoch[31/70], Step[61500], Loss: 0.0132, lr: 0.000006\n",
      "Epoch[31/70], Step[61600], Loss: 0.0014, lr: 0.000006\n",
      "Epoch[31/70], Step[61700], Loss: 0.0558, lr: 0.000006\n",
      "Epoch[31/70], Step[61800], Loss: 0.0159, lr: 0.000006\n",
      "Epoch[31/70], Step[61900], Loss: 0.0506, lr: 0.000006\n",
      "Epoch[32/70], Step[62000], Loss: 0.0496, lr: 0.000006\n",
      "Epoch[32/70], Step[62100], Loss: 0.0073, lr: 0.000006\n",
      "Epoch[32/70], Step[62200], Loss: 0.0023, lr: 0.000006\n",
      "Epoch[32/70], Step[62300], Loss: 0.0140, lr: 0.000006\n",
      "Epoch[32/70], Step[62400], Loss: 0.0174, lr: 0.000006\n",
      "Epoch[32/70], Step[62500], Loss: 0.0023, lr: 0.000006\n",
      "Epoch[32/70], Step[62600], Loss: 0.0324, lr: 0.000006\n",
      "Epoch[32/70], Step[62700], Loss: 0.0005, lr: 0.000006\n",
      "Epoch[32/70], Step[62800], Loss: 0.1057, lr: 0.000006\n",
      "Epoch[32/70], Step[62900], Loss: 0.0107, lr: 0.000006\n",
      "Epoch[32/70], Step[63000], Loss: 0.0030, lr: 0.000006\n",
      "Epoch[32/70], Step[63100], Loss: 0.0603, lr: 0.000006\n",
      "Epoch[32/70], Step[63200], Loss: 0.0074, lr: 0.000006\n",
      "Epoch[32/70], Step[63300], Loss: 0.0065, lr: 0.000006\n",
      "Epoch[32/70], Step[63400], Loss: 0.0128, lr: 0.000006\n",
      "Epoch[32/70], Step[63500], Loss: 0.0057, lr: 0.000006\n",
      "Epoch[32/70], Step[63600], Loss: 0.0180, lr: 0.000006\n",
      "Epoch[32/70], Step[63700], Loss: 0.0089, lr: 0.000006\n",
      "Epoch[32/70], Step[63800], Loss: 0.0281, lr: 0.000006\n",
      "Epoch[32/70], Step[63900], Loss: 0.0731, lr: 0.000006\n",
      "Epoch[33/70], Step[64000], Loss: 0.0556, lr: 0.000006\n",
      "Epoch[33/70], Step[64100], Loss: 0.0007, lr: 0.000006\n",
      "Epoch[33/70], Step[64200], Loss: 0.0204, lr: 0.000006\n",
      "Epoch[33/70], Step[64300], Loss: 0.0118, lr: 0.000006\n",
      "Epoch[33/70], Step[64400], Loss: 0.0071, lr: 0.000006\n",
      "Epoch[33/70], Step[64500], Loss: 0.0174, lr: 0.000006\n",
      "Epoch[33/70], Step[64600], Loss: 0.0533, lr: 0.000006\n",
      "Epoch[33/70], Step[64700], Loss: 0.0010, lr: 0.000006\n",
      "Epoch[33/70], Step[64800], Loss: 0.0436, lr: 0.000006\n",
      "Epoch[33/70], Step[64900], Loss: 0.0199, lr: 0.000006\n",
      "Epoch[33/70], Step[65000], Loss: 0.0103, lr: 0.000006\n",
      "Epoch[33/70], Step[65100], Loss: 0.0186, lr: 0.000006\n",
      "Epoch[33/70], Step[65200], Loss: 0.0258, lr: 0.000006\n",
      "Epoch[33/70], Step[65300], Loss: 0.0003, lr: 0.000006\n",
      "Epoch[33/70], Step[65400], Loss: 0.0030, lr: 0.000006\n",
      "Epoch[33/70], Step[65500], Loss: 0.0038, lr: 0.000006\n",
      "Epoch[33/70], Step[65600], Loss: 0.0044, lr: 0.000005\n",
      "Epoch[33/70], Step[65700], Loss: 0.0278, lr: 0.000005\n",
      "Epoch[33/70], Step[65800], Loss: 0.0730, lr: 0.000005\n",
      "Epoch[33/70], Step[65900], Loss: 0.0772, lr: 0.000005\n",
      "Epoch[34/70], Step[66000], Loss: 0.0017, lr: 0.000005\n",
      "Epoch[34/70], Step[66100], Loss: 0.0186, lr: 0.000005\n",
      "Epoch[34/70], Step[66200], Loss: 0.0024, lr: 0.000005\n",
      "Epoch[34/70], Step[66300], Loss: 0.0050, lr: 0.000005\n",
      "Epoch[34/70], Step[66400], Loss: 0.0199, lr: 0.000005\n",
      "Epoch[34/70], Step[66500], Loss: 0.0015, lr: 0.000005\n",
      "Epoch[34/70], Step[66600], Loss: 0.0825, lr: 0.000005\n",
      "Epoch[34/70], Step[66700], Loss: 0.0254, lr: 0.000005\n",
      "Epoch[34/70], Step[66800], Loss: 0.0577, lr: 0.000005\n",
      "Epoch[34/70], Step[66900], Loss: 0.0201, lr: 0.000005\n",
      "Epoch[34/70], Step[67000], Loss: 0.0055, lr: 0.000005\n",
      "Epoch[34/70], Step[67100], Loss: 0.0196, lr: 0.000005\n",
      "Epoch[34/70], Step[67200], Loss: 0.0371, lr: 0.000005\n",
      "Epoch[34/70], Step[67300], Loss: 0.0055, lr: 0.000005\n",
      "Epoch[34/70], Step[67400], Loss: 0.0063, lr: 0.000005\n",
      "Epoch[34/70], Step[67500], Loss: 0.0103, lr: 0.000005\n",
      "Epoch[34/70], Step[67600], Loss: 0.0080, lr: 0.000005\n",
      "Epoch[34/70], Step[67700], Loss: 0.0703, lr: 0.000005\n",
      "Epoch[34/70], Step[67800], Loss: 0.0089, lr: 0.000005\n",
      "Epoch[34/70], Step[67900], Loss: 0.0456, lr: 0.000005\n",
      "Epoch[35/70], Step[68000], Loss: 0.0161, lr: 0.000005\n",
      "Epoch[35/70], Step[68100], Loss: 0.0011, lr: 0.000005\n",
      "Epoch[35/70], Step[68200], Loss: 0.0031, lr: 0.000005\n",
      "Epoch[35/70], Step[68300], Loss: 0.0111, lr: 0.000005\n",
      "Epoch[35/70], Step[68400], Loss: 0.0053, lr: 0.000005\n",
      "Epoch[35/70], Step[68500], Loss: 0.0107, lr: 0.000005\n",
      "Epoch[35/70], Step[68600], Loss: 0.0016, lr: 0.000005\n",
      "Epoch[35/70], Step[68700], Loss: 0.0654, lr: 0.000005\n",
      "Epoch[35/70], Step[68800], Loss: 0.0216, lr: 0.000005\n",
      "Epoch[35/70], Step[68900], Loss: 0.0093, lr: 0.000005\n",
      "Epoch[35/70], Step[69000], Loss: 0.0108, lr: 0.000005\n",
      "Epoch[35/70], Step[69100], Loss: 0.0142, lr: 0.000005\n",
      "Epoch[35/70], Step[69200], Loss: 0.0836, lr: 0.000005\n",
      "Epoch[35/70], Step[69300], Loss: 0.0021, lr: 0.000005\n",
      "Epoch[35/70], Step[69400], Loss: 0.0119, lr: 0.000005\n",
      "Epoch[35/70], Step[69500], Loss: 0.0011, lr: 0.000005\n",
      "Epoch[35/70], Step[69600], Loss: 0.0113, lr: 0.000005\n",
      "Epoch[35/70], Step[69700], Loss: 0.0073, lr: 0.000005\n",
      "Epoch[35/70], Step[69800], Loss: 0.0124, lr: 0.000005\n",
      "Epoch[35/70], Step[69900], Loss: 0.0169, lr: 0.000005\n",
      "Epoch[36/70], Step[70000], Loss: 0.0449, lr: 0.000005\n",
      "Epoch[36/70], Step[70100], Loss: 0.0107, lr: 0.000005\n",
      "Epoch[36/70], Step[70200], Loss: 0.0256, lr: 0.000005\n",
      "Epoch[36/70], Step[70300], Loss: 0.0149, lr: 0.000005\n",
      "Epoch[36/70], Step[70400], Loss: 0.0103, lr: 0.000005\n",
      "Epoch[36/70], Step[70500], Loss: 0.0116, lr: 0.000005\n",
      "Epoch[36/70], Step[70600], Loss: 0.0378, lr: 0.000005\n",
      "Epoch[36/70], Step[70700], Loss: 0.0016, lr: 0.000005\n",
      "Epoch[36/70], Step[70800], Loss: 0.0794, lr: 0.000005\n",
      "Epoch[36/70], Step[70900], Loss: 0.0131, lr: 0.000005\n",
      "Epoch[36/70], Step[71000], Loss: 0.0018, lr: 0.000005\n",
      "Epoch[36/70], Step[71100], Loss: 0.0165, lr: 0.000005\n",
      "Epoch[36/70], Step[71200], Loss: 0.0380, lr: 0.000005\n",
      "Epoch[36/70], Step[71300], Loss: 0.0164, lr: 0.000005\n",
      "Epoch[36/70], Step[71400], Loss: 0.0130, lr: 0.000005\n",
      "Epoch[36/70], Step[71500], Loss: 0.0190, lr: 0.000005\n",
      "Epoch[36/70], Step[71600], Loss: 0.0083, lr: 0.000005\n",
      "Epoch[36/70], Step[71700], Loss: 0.0026, lr: 0.000005\n",
      "Epoch[36/70], Step[71800], Loss: 0.0186, lr: 0.000005\n",
      "Epoch[36/70], Step[71900], Loss: 0.0012, lr: 0.000005\n",
      "Epoch[37/70], Step[72000], Loss: 0.0037, lr: 0.000005\n",
      "Epoch[37/70], Step[72100], Loss: 0.0221, lr: 0.000005\n",
      "Epoch[37/70], Step[72200], Loss: 0.0230, lr: 0.000005\n",
      "Epoch[37/70], Step[72300], Loss: 0.0011, lr: 0.000005\n",
      "Epoch[37/70], Step[72400], Loss: 0.0176, lr: 0.000005\n",
      "Epoch[37/70], Step[72500], Loss: 0.0095, lr: 0.000005\n",
      "Epoch[37/70], Step[72600], Loss: 0.0755, lr: 0.000005\n",
      "Epoch[37/70], Step[72700], Loss: 0.0841, lr: 0.000005\n",
      "Epoch[37/70], Step[72800], Loss: 0.0633, lr: 0.000005\n",
      "Epoch[37/70], Step[72900], Loss: 0.0100, lr: 0.000005\n",
      "Epoch[37/70], Step[73000], Loss: 0.0045, lr: 0.000005\n",
      "Epoch[37/70], Step[73100], Loss: 0.0128, lr: 0.000005\n",
      "Epoch[37/70], Step[73200], Loss: 0.0663, lr: 0.000005\n",
      "Epoch[37/70], Step[73300], Loss: 0.0067, lr: 0.000005\n",
      "Epoch[37/70], Step[73400], Loss: 0.0136, lr: 0.000005\n",
      "Epoch[37/70], Step[73500], Loss: 0.0032, lr: 0.000005\n",
      "Epoch[37/70], Step[73600], Loss: 0.0029, lr: 0.000005\n",
      "Epoch[37/70], Step[73700], Loss: 0.0248, lr: 0.000005\n",
      "Epoch[37/70], Step[73800], Loss: 0.0726, lr: 0.000005\n",
      "Epoch[37/70], Step[73900], Loss: 0.0334, lr: 0.000005\n",
      "Epoch[38/70], Step[74000], Loss: 0.0543, lr: 0.000005\n",
      "Epoch[38/70], Step[74100], Loss: 0.0176, lr: 0.000005\n",
      "Epoch[38/70], Step[74200], Loss: 0.0037, lr: 0.000005\n",
      "Epoch[38/70], Step[74300], Loss: 0.0068, lr: 0.000005\n",
      "Epoch[38/70], Step[74400], Loss: 0.0018, lr: 0.000005\n",
      "Epoch[38/70], Step[74500], Loss: 0.0190, lr: 0.000005\n",
      "Epoch[38/70], Step[74600], Loss: 0.0239, lr: 0.000004\n",
      "Epoch[38/70], Step[74700], Loss: 0.0130, lr: 0.000004\n",
      "Epoch[38/70], Step[74800], Loss: 0.1375, lr: 0.000004\n",
      "Epoch[38/70], Step[74900], Loss: 0.0138, lr: 0.000004\n",
      "Epoch[38/70], Step[75000], Loss: 0.0046, lr: 0.000004\n",
      "Epoch[38/70], Step[75100], Loss: 0.0700, lr: 0.000004\n",
      "Epoch[38/70], Step[75200], Loss: 0.0712, lr: 0.000004\n",
      "Epoch[38/70], Step[75300], Loss: 0.0006, lr: 0.000004\n",
      "Epoch[38/70], Step[75400], Loss: 0.0129, lr: 0.000004\n",
      "Epoch[38/70], Step[75500], Loss: 0.0081, lr: 0.000004\n",
      "Epoch[38/70], Step[75600], Loss: 0.0080, lr: 0.000004\n",
      "Epoch[38/70], Step[75700], Loss: 0.0507, lr: 0.000004\n",
      "Epoch[38/70], Step[75800], Loss: 0.0802, lr: 0.000004\n",
      "Epoch[38/70], Step[75900], Loss: 0.0403, lr: 0.000004\n",
      "Epoch[39/70], Step[76000], Loss: 0.0371, lr: 0.000004\n",
      "Epoch[39/70], Step[76100], Loss: 0.0263, lr: 0.000004\n",
      "Epoch[39/70], Step[76200], Loss: 0.0246, lr: 0.000004\n",
      "Epoch[39/70], Step[76300], Loss: 0.0062, lr: 0.000004\n",
      "Epoch[39/70], Step[76400], Loss: 0.0027, lr: 0.000004\n",
      "Epoch[39/70], Step[76500], Loss: 0.0025, lr: 0.000004\n",
      "Epoch[39/70], Step[76600], Loss: 0.0478, lr: 0.000004\n",
      "Epoch[39/70], Step[76700], Loss: 0.0039, lr: 0.000004\n",
      "Epoch[39/70], Step[76800], Loss: 0.0466, lr: 0.000004\n",
      "Epoch[39/70], Step[76900], Loss: 0.0042, lr: 0.000004\n",
      "Epoch[39/70], Step[77000], Loss: 0.0085, lr: 0.000004\n",
      "Epoch[39/70], Step[77100], Loss: 0.0395, lr: 0.000004\n",
      "Epoch[39/70], Step[77200], Loss: 0.0044, lr: 0.000004\n",
      "Epoch[39/70], Step[77300], Loss: 0.0018, lr: 0.000004\n",
      "Epoch[39/70], Step[77400], Loss: 0.0012, lr: 0.000004\n",
      "Epoch[39/70], Step[77500], Loss: 0.0113, lr: 0.000004\n",
      "Epoch[39/70], Step[77600], Loss: 0.0160, lr: 0.000004\n",
      "Epoch[39/70], Step[77700], Loss: 0.0878, lr: 0.000004\n",
      "Epoch[39/70], Step[77800], Loss: 0.0148, lr: 0.000004\n",
      "Epoch[39/70], Step[77900], Loss: 0.0386, lr: 0.000004\n",
      "Epoch[40/70], Step[78000], Loss: 0.0116, lr: 0.000004\n",
      "Epoch[40/70], Step[78100], Loss: 0.0048, lr: 0.000004\n",
      "Epoch[40/70], Step[78200], Loss: 0.0202, lr: 0.000004\n",
      "Epoch[40/70], Step[78300], Loss: 0.0022, lr: 0.000004\n",
      "Epoch[40/70], Step[78400], Loss: 0.0008, lr: 0.000004\n",
      "Epoch[40/70], Step[78500], Loss: 0.0013, lr: 0.000004\n",
      "Epoch[40/70], Step[78600], Loss: 0.0904, lr: 0.000004\n",
      "Epoch[40/70], Step[78700], Loss: 0.0060, lr: 0.000004\n",
      "Epoch[40/70], Step[78800], Loss: 0.0755, lr: 0.000004\n",
      "Epoch[40/70], Step[78900], Loss: 0.0117, lr: 0.000004\n",
      "Epoch[40/70], Step[79000], Loss: 0.0026, lr: 0.000004\n",
      "Epoch[40/70], Step[79100], Loss: 0.0356, lr: 0.000004\n",
      "Epoch[40/70], Step[79200], Loss: 0.0166, lr: 0.000004\n",
      "Epoch[40/70], Step[79300], Loss: 0.0135, lr: 0.000004\n",
      "Epoch[40/70], Step[79400], Loss: 0.0013, lr: 0.000004\n",
      "Epoch[40/70], Step[79500], Loss: 0.0099, lr: 0.000004\n",
      "Epoch[40/70], Step[79600], Loss: 0.0047, lr: 0.000004\n",
      "Epoch[40/70], Step[79700], Loss: 0.0689, lr: 0.000004\n",
      "Epoch[40/70], Step[79800], Loss: 0.0220, lr: 0.000004\n",
      "Epoch[40/70], Step[79900], Loss: 0.0405, lr: 0.000004\n",
      "Epoch[41/70], Step[80000], Loss: 0.0295, lr: 0.000004\n",
      "Epoch[41/70], Step[80100], Loss: 0.0141, lr: 0.000004\n",
      "Epoch[41/70], Step[80200], Loss: 0.0092, lr: 0.000004\n",
      "Epoch[41/70], Step[80300], Loss: 0.0006, lr: 0.000004\n",
      "Epoch[41/70], Step[80400], Loss: 0.0227, lr: 0.000004\n",
      "Epoch[41/70], Step[80500], Loss: 0.0183, lr: 0.000004\n",
      "Epoch[41/70], Step[80600], Loss: 0.0851, lr: 0.000004\n",
      "Epoch[41/70], Step[80700], Loss: 0.0805, lr: 0.000004\n",
      "Epoch[41/70], Step[80800], Loss: 0.1047, lr: 0.000004\n",
      "Epoch[41/70], Step[80900], Loss: 0.0017, lr: 0.000004\n",
      "Epoch[41/70], Step[81000], Loss: 0.0064, lr: 0.000004\n",
      "Epoch[41/70], Step[81100], Loss: 0.0252, lr: 0.000004\n",
      "Epoch[41/70], Step[81200], Loss: 0.0885, lr: 0.000004\n",
      "Epoch[41/70], Step[81300], Loss: 0.0112, lr: 0.000004\n",
      "Epoch[41/70], Step[81400], Loss: 0.0027, lr: 0.000004\n",
      "Epoch[41/70], Step[81500], Loss: 0.0076, lr: 0.000004\n",
      "Epoch[41/70], Step[81600], Loss: 0.0028, lr: 0.000004\n",
      "Epoch[41/70], Step[81700], Loss: 0.0023, lr: 0.000004\n",
      "Epoch[41/70], Step[81800], Loss: 0.0739, lr: 0.000004\n",
      "Epoch[41/70], Step[81900], Loss: 0.0091, lr: 0.000004\n",
      "Epoch[42/70], Step[82000], Loss: 0.0022, lr: 0.000004\n",
      "Epoch[42/70], Step[82100], Loss: 0.0014, lr: 0.000004\n",
      "Epoch[42/70], Step[82200], Loss: 0.0025, lr: 0.000004\n",
      "Epoch[42/70], Step[82300], Loss: 0.0140, lr: 0.000004\n",
      "Epoch[42/70], Step[82400], Loss: 0.0032, lr: 0.000004\n",
      "Epoch[42/70], Step[82500], Loss: 0.0245, lr: 0.000004\n",
      "Epoch[42/70], Step[82600], Loss: 0.0595, lr: 0.000004\n",
      "Epoch[42/70], Step[82700], Loss: 0.0043, lr: 0.000004\n",
      "Epoch[42/70], Step[82800], Loss: 0.1080, lr: 0.000004\n",
      "Epoch[42/70], Step[82900], Loss: 0.0033, lr: 0.000004\n",
      "Epoch[42/70], Step[83000], Loss: 0.0250, lr: 0.000004\n",
      "Epoch[42/70], Step[83100], Loss: 0.0222, lr: 0.000004\n",
      "Epoch[42/70], Step[83200], Loss: 0.0069, lr: 0.000004\n",
      "Epoch[42/70], Step[83300], Loss: 0.0079, lr: 0.000004\n",
      "Epoch[42/70], Step[83400], Loss: 0.0083, lr: 0.000004\n",
      "Epoch[42/70], Step[83500], Loss: 0.0018, lr: 0.000004\n",
      "Epoch[42/70], Step[83600], Loss: 0.0026, lr: 0.000004\n",
      "Epoch[42/70], Step[83700], Loss: 0.0880, lr: 0.000003\n",
      "Epoch[42/70], Step[83800], Loss: 0.0080, lr: 0.000003\n",
      "Epoch[42/70], Step[83900], Loss: 0.0744, lr: 0.000003\n",
      "Epoch[43/70], Step[84000], Loss: 0.0605, lr: 0.000003\n",
      "Epoch[43/70], Step[84100], Loss: 0.0150, lr: 0.000003\n",
      "Epoch[43/70], Step[84200], Loss: 0.0258, lr: 0.000003\n",
      "Epoch[43/70], Step[84300], Loss: 0.0013, lr: 0.000003\n",
      "Epoch[43/70], Step[84400], Loss: 0.0086, lr: 0.000003\n",
      "Epoch[43/70], Step[84500], Loss: 0.0078, lr: 0.000003\n",
      "Epoch[43/70], Step[84600], Loss: 0.0255, lr: 0.000003\n",
      "Epoch[43/70], Step[84700], Loss: 0.0642, lr: 0.000003\n",
      "Epoch[43/70], Step[84800], Loss: 0.0615, lr: 0.000003\n",
      "Epoch[43/70], Step[84900], Loss: 0.0023, lr: 0.000003\n",
      "Epoch[43/70], Step[85000], Loss: 0.0223, lr: 0.000003\n",
      "Epoch[43/70], Step[85100], Loss: 0.0078, lr: 0.000003\n",
      "Epoch[43/70], Step[85200], Loss: 0.0076, lr: 0.000003\n",
      "Epoch[43/70], Step[85300], Loss: 0.0080, lr: 0.000003\n",
      "Epoch[43/70], Step[85400], Loss: 0.0166, lr: 0.000003\n",
      "Epoch[43/70], Step[85500], Loss: 0.0015, lr: 0.000003\n",
      "Epoch[43/70], Step[85600], Loss: 0.0043, lr: 0.000003\n",
      "Epoch[43/70], Step[85700], Loss: 0.0221, lr: 0.000003\n",
      "Epoch[43/70], Step[85800], Loss: 0.0086, lr: 0.000003\n",
      "Epoch[43/70], Step[85900], Loss: 0.0016, lr: 0.000003\n",
      "Epoch[44/70], Step[86000], Loss: 0.0106, lr: 0.000003\n",
      "Epoch[44/70], Step[86100], Loss: 0.0145, lr: 0.000003\n",
      "Epoch[44/70], Step[86200], Loss: 0.0255, lr: 0.000003\n",
      "Epoch[44/70], Step[86300], Loss: 0.0084, lr: 0.000003\n",
      "Epoch[44/70], Step[86400], Loss: 0.0235, lr: 0.000003\n",
      "Epoch[44/70], Step[86500], Loss: 0.0098, lr: 0.000003\n",
      "Epoch[44/70], Step[86600], Loss: 0.0902, lr: 0.000003\n",
      "Epoch[44/70], Step[86700], Loss: 0.0706, lr: 0.000003\n",
      "Epoch[44/70], Step[86800], Loss: 0.1671, lr: 0.000003\n",
      "Epoch[44/70], Step[86900], Loss: 0.0091, lr: 0.000003\n",
      "Epoch[44/70], Step[87000], Loss: 0.0015, lr: 0.000003\n",
      "Epoch[44/70], Step[87100], Loss: 0.0166, lr: 0.000003\n",
      "Epoch[44/70], Step[87200], Loss: 0.0647, lr: 0.000003\n",
      "Epoch[44/70], Step[87300], Loss: 0.0009, lr: 0.000003\n",
      "Epoch[44/70], Step[87400], Loss: 0.0005, lr: 0.000003\n",
      "Epoch[44/70], Step[87500], Loss: 0.0011, lr: 0.000003\n",
      "Epoch[44/70], Step[87600], Loss: 0.0105, lr: 0.000003\n",
      "Epoch[44/70], Step[87700], Loss: 0.0804, lr: 0.000003\n",
      "Epoch[44/70], Step[87800], Loss: 0.0579, lr: 0.000003\n",
      "Epoch[44/70], Step[87900], Loss: 0.0678, lr: 0.000003\n",
      "Epoch[45/70], Step[88000], Loss: 0.0822, lr: 0.000003\n",
      "Epoch[45/70], Step[88100], Loss: 0.0052, lr: 0.000003\n",
      "Epoch[45/70], Step[88200], Loss: 0.0024, lr: 0.000003\n",
      "Epoch[45/70], Step[88300], Loss: 0.0064, lr: 0.000003\n",
      "Epoch[45/70], Step[88400], Loss: 0.0181, lr: 0.000003\n",
      "Epoch[45/70], Step[88500], Loss: 0.0037, lr: 0.000003\n",
      "Epoch[45/70], Step[88600], Loss: 0.0868, lr: 0.000003\n",
      "Epoch[45/70], Step[88700], Loss: 0.0039, lr: 0.000003\n",
      "Epoch[45/70], Step[88800], Loss: 0.0616, lr: 0.000003\n",
      "Epoch[45/70], Step[88900], Loss: 0.0148, lr: 0.000003\n",
      "Epoch[45/70], Step[89000], Loss: 0.0198, lr: 0.000003\n",
      "Epoch[45/70], Step[89100], Loss: 0.0765, lr: 0.000003\n",
      "Epoch[45/70], Step[89200], Loss: 0.0180, lr: 0.000003\n",
      "Epoch[45/70], Step[89300], Loss: 0.0037, lr: 0.000003\n",
      "Epoch[45/70], Step[89400], Loss: 0.0010, lr: 0.000003\n",
      "Epoch[45/70], Step[89500], Loss: 0.0029, lr: 0.000003\n",
      "Epoch[45/70], Step[89600], Loss: 0.0149, lr: 0.000003\n",
      "Epoch[45/70], Step[89700], Loss: 0.0033, lr: 0.000003\n",
      "Epoch[45/70], Step[89800], Loss: 0.0723, lr: 0.000003\n",
      "Epoch[45/70], Step[89900], Loss: 0.0012, lr: 0.000003\n",
      "Epoch[46/70], Step[90000], Loss: 0.0043, lr: 0.000003\n",
      "Epoch[46/70], Step[90100], Loss: 0.0137, lr: 0.000003\n",
      "Epoch[46/70], Step[90200], Loss: 0.0262, lr: 0.000003\n",
      "Epoch[46/70], Step[90300], Loss: 0.0054, lr: 0.000003\n",
      "Epoch[46/70], Step[90400], Loss: 0.0025, lr: 0.000003\n",
      "Epoch[46/70], Step[90500], Loss: 0.0249, lr: 0.000003\n",
      "Epoch[46/70], Step[90600], Loss: 0.0075, lr: 0.000003\n",
      "Epoch[46/70], Step[90700], Loss: 0.0854, lr: 0.000003\n",
      "Epoch[46/70], Step[90800], Loss: 0.0998, lr: 0.000003\n",
      "Epoch[46/70], Step[90900], Loss: 0.0047, lr: 0.000003\n",
      "Epoch[46/70], Step[91000], Loss: 0.0010, lr: 0.000003\n",
      "Epoch[46/70], Step[91100], Loss: 0.0565, lr: 0.000003\n",
      "Epoch[46/70], Step[91200], Loss: 0.0133, lr: 0.000003\n",
      "Epoch[46/70], Step[91300], Loss: 0.0012, lr: 0.000003\n",
      "Epoch[46/70], Step[91400], Loss: 0.0160, lr: 0.000003\n",
      "Epoch[46/70], Step[91500], Loss: 0.0116, lr: 0.000003\n",
      "Epoch[46/70], Step[91600], Loss: 0.0155, lr: 0.000003\n",
      "Epoch[46/70], Step[91700], Loss: 0.0460, lr: 0.000003\n",
      "Epoch[46/70], Step[91800], Loss: 0.0010, lr: 0.000003\n",
      "Epoch[46/70], Step[91900], Loss: 0.0069, lr: 0.000003\n",
      "Epoch[47/70], Step[92000], Loss: 0.0410, lr: 0.000003\n",
      "Epoch[47/70], Step[92100], Loss: 0.0191, lr: 0.000003\n",
      "Epoch[47/70], Step[92200], Loss: 0.0026, lr: 0.000003\n",
      "Epoch[47/70], Step[92300], Loss: 0.0046, lr: 0.000003\n",
      "Epoch[47/70], Step[92400], Loss: 0.0189, lr: 0.000003\n",
      "Epoch[47/70], Step[92500], Loss: 0.0024, lr: 0.000003\n",
      "Epoch[47/70], Step[92600], Loss: 0.0017, lr: 0.000003\n",
      "Epoch[47/70], Step[92700], Loss: 0.0005, lr: 0.000003\n",
      "Epoch[47/70], Step[92800], Loss: 0.0732, lr: 0.000003\n",
      "Epoch[47/70], Step[92900], Loss: 0.0177, lr: 0.000003\n",
      "Epoch[47/70], Step[93000], Loss: 0.0028, lr: 0.000003\n",
      "Epoch[47/70], Step[93100], Loss: 0.0503, lr: 0.000003\n",
      "Epoch[47/70], Step[93200], Loss: 0.0712, lr: 0.000003\n",
      "Epoch[47/70], Step[93300], Loss: 0.0164, lr: 0.000003\n",
      "Epoch[47/70], Step[93400], Loss: 0.0023, lr: 0.000002\n",
      "Epoch[47/70], Step[93500], Loss: 0.0022, lr: 0.000002\n",
      "Epoch[47/70], Step[93600], Loss: 0.0003, lr: 0.000002\n",
      "Epoch[47/70], Step[93700], Loss: 0.0612, lr: 0.000002\n",
      "Epoch[47/70], Step[93800], Loss: 0.0028, lr: 0.000002\n",
      "Epoch[47/70], Step[93900], Loss: 0.0621, lr: 0.000002\n",
      "Epoch[48/70], Step[94000], Loss: 0.0828, lr: 0.000002\n",
      "Epoch[48/70], Step[94100], Loss: 0.0242, lr: 0.000002\n",
      "Epoch[48/70], Step[94200], Loss: 0.0129, lr: 0.000002\n",
      "Epoch[48/70], Step[94300], Loss: 0.0136, lr: 0.000002\n",
      "Epoch[48/70], Step[94400], Loss: 0.0192, lr: 0.000002\n",
      "Epoch[48/70], Step[94500], Loss: 0.0087, lr: 0.000002\n",
      "Epoch[48/70], Step[94600], Loss: 0.0011, lr: 0.000002\n",
      "Epoch[48/70], Step[94700], Loss: 0.0013, lr: 0.000002\n",
      "Epoch[48/70], Step[94800], Loss: 0.0494, lr: 0.000002\n",
      "Epoch[48/70], Step[94900], Loss: 0.0213, lr: 0.000002\n",
      "Epoch[48/70], Step[95000], Loss: 0.0267, lr: 0.000002\n",
      "Epoch[48/70], Step[95100], Loss: 0.0441, lr: 0.000002\n",
      "Epoch[48/70], Step[95200], Loss: 0.0054, lr: 0.000002\n",
      "Epoch[48/70], Step[95300], Loss: 0.0172, lr: 0.000002\n",
      "Epoch[48/70], Step[95400], Loss: 0.0073, lr: 0.000002\n",
      "Epoch[48/70], Step[95500], Loss: 0.0127, lr: 0.000002\n",
      "Epoch[48/70], Step[95600], Loss: 0.0037, lr: 0.000002\n",
      "Epoch[48/70], Step[95700], Loss: 0.0293, lr: 0.000002\n",
      "Epoch[48/70], Step[95800], Loss: 0.0721, lr: 0.000002\n",
      "Epoch[48/70], Step[95900], Loss: 0.0279, lr: 0.000002\n",
      "Epoch[49/70], Step[96000], Loss: 0.0130, lr: 0.000002\n",
      "Epoch[49/70], Step[96100], Loss: 0.0199, lr: 0.000002\n",
      "Epoch[49/70], Step[96200], Loss: 0.0124, lr: 0.000002\n",
      "Epoch[49/70], Step[96300], Loss: 0.0014, lr: 0.000002\n",
      "Epoch[49/70], Step[96400], Loss: 0.0207, lr: 0.000002\n",
      "Epoch[49/70], Step[96500], Loss: 0.0082, lr: 0.000002\n",
      "Epoch[49/70], Step[96600], Loss: 0.0007, lr: 0.000002\n",
      "Epoch[49/70], Step[96700], Loss: 0.0013, lr: 0.000002\n",
      "Epoch[49/70], Step[96800], Loss: 0.0407, lr: 0.000002\n",
      "Epoch[49/70], Step[96900], Loss: 0.0065, lr: 0.000002\n",
      "Epoch[49/70], Step[97000], Loss: 0.0085, lr: 0.000002\n",
      "Epoch[49/70], Step[97100], Loss: 0.0702, lr: 0.000002\n",
      "Epoch[49/70], Step[97200], Loss: 0.0007, lr: 0.000002\n",
      "Epoch[49/70], Step[97300], Loss: 0.0089, lr: 0.000002\n",
      "Epoch[49/70], Step[97400], Loss: 0.0178, lr: 0.000002\n",
      "Epoch[49/70], Step[97500], Loss: 0.0028, lr: 0.000002\n",
      "Epoch[49/70], Step[97600], Loss: 0.0021, lr: 0.000002\n",
      "Epoch[49/70], Step[97700], Loss: 0.0009, lr: 0.000002\n",
      "Epoch[49/70], Step[97800], Loss: 0.0068, lr: 0.000002\n",
      "Epoch[49/70], Step[97900], Loss: 0.0017, lr: 0.000002\n",
      "Epoch[50/70], Step[98000], Loss: 0.0664, lr: 0.000002\n",
      "Epoch[50/70], Step[98100], Loss: 0.0167, lr: 0.000002\n",
      "Epoch[50/70], Step[98200], Loss: 0.0028, lr: 0.000002\n",
      "Epoch[50/70], Step[98300], Loss: 0.0007, lr: 0.000002\n",
      "Epoch[50/70], Step[98400], Loss: 0.0234, lr: 0.000002\n",
      "Epoch[50/70], Step[98500], Loss: 0.0004, lr: 0.000002\n",
      "Epoch[50/70], Step[98600], Loss: 0.0022, lr: 0.000002\n",
      "Epoch[50/70], Step[98700], Loss: 0.0261, lr: 0.000002\n",
      "Epoch[50/70], Step[98800], Loss: 0.1088, lr: 0.000002\n",
      "Epoch[50/70], Step[98900], Loss: 0.0028, lr: 0.000002\n",
      "Epoch[50/70], Step[99000], Loss: 0.0262, lr: 0.000002\n",
      "Epoch[50/70], Step[99100], Loss: 0.0669, lr: 0.000002\n",
      "Epoch[50/70], Step[99200], Loss: 0.0638, lr: 0.000002\n",
      "Epoch[50/70], Step[99300], Loss: 0.0043, lr: 0.000002\n",
      "Epoch[50/70], Step[99400], Loss: 0.0056, lr: 0.000002\n",
      "Epoch[50/70], Step[99500], Loss: 0.0085, lr: 0.000002\n",
      "Epoch[50/70], Step[99600], Loss: 0.0084, lr: 0.000002\n",
      "Epoch[50/70], Step[99700], Loss: 0.0009, lr: 0.000002\n",
      "Epoch[50/70], Step[99800], Loss: 0.0118, lr: 0.000002\n",
      "Epoch[50/70], Step[99900], Loss: 0.0751, lr: 0.000002\n",
      "Epoch[51/70], Step[100000], Loss: 0.0667, lr: 0.000002\n",
      "Epoch[51/70], Step[100100], Loss: 0.0151, lr: 0.000002\n",
      "Epoch[51/70], Step[100200], Loss: 0.0024, lr: 0.000002\n",
      "Epoch[51/70], Step[100300], Loss: 0.0084, lr: 0.000002\n",
      "Epoch[51/70], Step[100400], Loss: 0.0121, lr: 0.000002\n",
      "Epoch[51/70], Step[100500], Loss: 0.0079, lr: 0.000002\n",
      "Epoch[51/70], Step[100600], Loss: 0.0902, lr: 0.000002\n",
      "Epoch[51/70], Step[100700], Loss: 0.0187, lr: 0.000002\n",
      "Epoch[51/70], Step[100800], Loss: 0.0772, lr: 0.000002\n",
      "Epoch[51/70], Step[100900], Loss: 0.0026, lr: 0.000002\n",
      "Epoch[51/70], Step[101000], Loss: 0.0107, lr: 0.000002\n",
      "Epoch[51/70], Step[101100], Loss: 0.0219, lr: 0.000002\n",
      "Epoch[51/70], Step[101200], Loss: 0.0232, lr: 0.000002\n",
      "Epoch[51/70], Step[101300], Loss: 0.0147, lr: 0.000002\n",
      "Epoch[51/70], Step[101400], Loss: 0.0057, lr: 0.000002\n",
      "Epoch[51/70], Step[101500], Loss: 0.0149, lr: 0.000002\n",
      "Epoch[51/70], Step[101600], Loss: 0.0186, lr: 0.000002\n",
      "Epoch[51/70], Step[101700], Loss: 0.0594, lr: 0.000002\n",
      "Epoch[51/70], Step[101800], Loss: 0.0452, lr: 0.000002\n",
      "Epoch[51/70], Step[101900], Loss: 0.0085, lr: 0.000002\n",
      "Epoch[52/70], Step[102000], Loss: 0.0099, lr: 0.000002\n",
      "Epoch[52/70], Step[102100], Loss: 0.0083, lr: 0.000002\n",
      "Epoch[52/70], Step[102200], Loss: 0.0254, lr: 0.000002\n",
      "Epoch[52/70], Step[102300], Loss: 0.0069, lr: 0.000002\n",
      "Epoch[52/70], Step[102400], Loss: 0.0187, lr: 0.000002\n",
      "Epoch[52/70], Step[102500], Loss: 0.0105, lr: 0.000002\n",
      "Epoch[52/70], Step[102600], Loss: 0.0187, lr: 0.000002\n",
      "Epoch[52/70], Step[102700], Loss: 0.0079, lr: 0.000002\n",
      "Epoch[52/70], Step[102800], Loss: 0.0615, lr: 0.000002\n",
      "Epoch[52/70], Step[102900], Loss: 0.0051, lr: 0.000002\n",
      "Epoch[52/70], Step[103000], Loss: 0.0117, lr: 0.000002\n",
      "Epoch[52/70], Step[103100], Loss: 0.0248, lr: 0.000002\n",
      "Epoch[52/70], Step[103200], Loss: 0.0694, lr: 0.000002\n",
      "Epoch[52/70], Step[103300], Loss: 0.0174, lr: 0.000002\n",
      "Epoch[52/70], Step[103400], Loss: 0.0064, lr: 0.000002\n",
      "Epoch[52/70], Step[103500], Loss: 0.0027, lr: 0.000002\n",
      "Epoch[52/70], Step[103600], Loss: 0.0017, lr: 0.000002\n",
      "Epoch[52/70], Step[103700], Loss: 0.0926, lr: 0.000002\n",
      "Epoch[52/70], Step[103800], Loss: 0.0290, lr: 0.000002\n",
      "Epoch[52/70], Step[103900], Loss: 0.0170, lr: 0.000002\n",
      "Epoch[53/70], Step[104000], Loss: 0.0877, lr: 0.000002\n",
      "Epoch[53/70], Step[104100], Loss: 0.0049, lr: 0.000002\n",
      "Epoch[53/70], Step[104200], Loss: 0.0242, lr: 0.000002\n",
      "Epoch[53/70], Step[104300], Loss: 0.0067, lr: 0.000002\n",
      "Epoch[53/70], Step[104400], Loss: 0.0028, lr: 0.000002\n",
      "Epoch[53/70], Step[104500], Loss: 0.0235, lr: 0.000002\n",
      "Epoch[53/70], Step[104600], Loss: 0.0650, lr: 0.000001\n",
      "Epoch[53/70], Step[104700], Loss: 0.0865, lr: 0.000001\n",
      "Epoch[53/70], Step[104800], Loss: 0.0178, lr: 0.000001\n",
      "Epoch[53/70], Step[104900], Loss: 0.0129, lr: 0.000001\n",
      "Epoch[53/70], Step[105000], Loss: 0.0250, lr: 0.000001\n",
      "Epoch[53/70], Step[105100], Loss: 0.0009, lr: 0.000001\n",
      "Epoch[53/70], Step[105200], Loss: 0.0483, lr: 0.000001\n",
      "Epoch[53/70], Step[105300], Loss: 0.0015, lr: 0.000001\n",
      "Epoch[53/70], Step[105400], Loss: 0.0049, lr: 0.000001\n",
      "Epoch[53/70], Step[105500], Loss: 0.0005, lr: 0.000001\n",
      "Epoch[53/70], Step[105600], Loss: 0.0164, lr: 0.000001\n",
      "Epoch[53/70], Step[105700], Loss: 0.0092, lr: 0.000001\n",
      "Epoch[53/70], Step[105800], Loss: 0.0631, lr: 0.000001\n",
      "Epoch[53/70], Step[105900], Loss: 0.0343, lr: 0.000001\n",
      "Epoch[54/70], Step[106000], Loss: 0.0130, lr: 0.000001\n",
      "Epoch[54/70], Step[106100], Loss: 0.0012, lr: 0.000001\n",
      "Epoch[54/70], Step[106200], Loss: 0.0257, lr: 0.000001\n",
      "Epoch[54/70], Step[106300], Loss: 0.0060, lr: 0.000001\n",
      "Epoch[54/70], Step[106400], Loss: 0.0212, lr: 0.000001\n",
      "Epoch[54/70], Step[106500], Loss: 0.0005, lr: 0.000001\n",
      "Epoch[54/70], Step[106600], Loss: 0.0397, lr: 0.000001\n",
      "Epoch[54/70], Step[106700], Loss: 0.0809, lr: 0.000001\n",
      "Epoch[54/70], Step[106800], Loss: 0.0012, lr: 0.000001\n",
      "Epoch[54/70], Step[106900], Loss: 0.0179, lr: 0.000001\n",
      "Epoch[54/70], Step[107000], Loss: 0.0063, lr: 0.000001\n",
      "Epoch[54/70], Step[107100], Loss: 0.0018, lr: 0.000001\n",
      "Epoch[54/70], Step[107200], Loss: 0.0222, lr: 0.000001\n",
      "Epoch[54/70], Step[107300], Loss: 0.0005, lr: 0.000001\n",
      "Epoch[54/70], Step[107400], Loss: 0.0010, lr: 0.000001\n",
      "Epoch[54/70], Step[107500], Loss: 0.0006, lr: 0.000001\n",
      "Epoch[54/70], Step[107600], Loss: 0.0078, lr: 0.000001\n",
      "Epoch[54/70], Step[107700], Loss: 0.0022, lr: 0.000001\n",
      "Epoch[54/70], Step[107800], Loss: 0.0126, lr: 0.000001\n",
      "Epoch[54/70], Step[107900], Loss: 0.0499, lr: 0.000001\n",
      "Epoch[55/70], Step[108000], Loss: 0.0326, lr: 0.000001\n",
      "Epoch[55/70], Step[108100], Loss: 0.0178, lr: 0.000001\n",
      "Epoch[55/70], Step[108200], Loss: 0.0017, lr: 0.000001\n",
      "Epoch[55/70], Step[108300], Loss: 0.0069, lr: 0.000001\n",
      "Epoch[55/70], Step[108400], Loss: 0.0058, lr: 0.000001\n",
      "Epoch[55/70], Step[108500], Loss: 0.0145, lr: 0.000001\n",
      "Epoch[55/70], Step[108600], Loss: 0.0741, lr: 0.000001\n",
      "Epoch[55/70], Step[108700], Loss: 0.0309, lr: 0.000001\n",
      "Epoch[55/70], Step[108800], Loss: 0.0183, lr: 0.000001\n",
      "Epoch[55/70], Step[108900], Loss: 0.0027, lr: 0.000001\n",
      "Epoch[55/70], Step[109000], Loss: 0.0019, lr: 0.000001\n",
      "Epoch[55/70], Step[109100], Loss: 0.0193, lr: 0.000001\n",
      "Epoch[55/70], Step[109200], Loss: 0.0009, lr: 0.000001\n",
      "Epoch[55/70], Step[109300], Loss: 0.0159, lr: 0.000001\n",
      "Epoch[55/70], Step[109400], Loss: 0.0024, lr: 0.000001\n",
      "Epoch[55/70], Step[109500], Loss: 0.0022, lr: 0.000001\n",
      "Epoch[55/70], Step[109600], Loss: 0.0089, lr: 0.000001\n",
      "Epoch[55/70], Step[109700], Loss: 0.0149, lr: 0.000001\n",
      "Epoch[55/70], Step[109800], Loss: 0.0351, lr: 0.000001\n",
      "Epoch[55/70], Step[109900], Loss: 0.0695, lr: 0.000001\n",
      "Epoch[56/70], Step[110000], Loss: 0.0630, lr: 0.000001\n",
      "Epoch[56/70], Step[110100], Loss: 0.0180, lr: 0.000001\n",
      "Epoch[56/70], Step[110200], Loss: 0.0073, lr: 0.000001\n",
      "Epoch[56/70], Step[110300], Loss: 0.0135, lr: 0.000001\n",
      "Epoch[56/70], Step[110400], Loss: 0.0196, lr: 0.000001\n",
      "Epoch[56/70], Step[110500], Loss: 0.0219, lr: 0.000001\n",
      "Epoch[56/70], Step[110600], Loss: 0.0040, lr: 0.000001\n",
      "Epoch[56/70], Step[110700], Loss: 0.0188, lr: 0.000001\n",
      "Epoch[56/70], Step[110800], Loss: 0.0150, lr: 0.000001\n",
      "Epoch[56/70], Step[110900], Loss: 0.0008, lr: 0.000001\n",
      "Epoch[56/70], Step[111000], Loss: 0.0010, lr: 0.000001\n",
      "Epoch[56/70], Step[111100], Loss: 0.0037, lr: 0.000001\n",
      "Epoch[56/70], Step[111200], Loss: 0.0017, lr: 0.000001\n",
      "Epoch[56/70], Step[111300], Loss: 0.0035, lr: 0.000001\n",
      "Epoch[56/70], Step[111400], Loss: 0.0013, lr: 0.000001\n",
      "Epoch[56/70], Step[111500], Loss: 0.0150, lr: 0.000001\n",
      "Epoch[56/70], Step[111600], Loss: 0.0164, lr: 0.000001\n",
      "Epoch[56/70], Step[111700], Loss: 0.0171, lr: 0.000001\n",
      "Epoch[56/70], Step[111800], Loss: 0.0701, lr: 0.000001\n",
      "Epoch[56/70], Step[111900], Loss: 0.0126, lr: 0.000001\n",
      "Epoch[57/70], Step[112000], Loss: 0.0420, lr: 0.000001\n",
      "Epoch[57/70], Step[112100], Loss: 0.0222, lr: 0.000001\n",
      "Epoch[57/70], Step[112200], Loss: 0.0008, lr: 0.000001\n",
      "Epoch[57/70], Step[112300], Loss: 0.0071, lr: 0.000001\n",
      "Epoch[57/70], Step[112400], Loss: 0.0012, lr: 0.000001\n",
      "Epoch[57/70], Step[112500], Loss: 0.0203, lr: 0.000001\n",
      "Epoch[57/70], Step[112600], Loss: 0.0268, lr: 0.000001\n",
      "Epoch[57/70], Step[112700], Loss: 0.0766, lr: 0.000001\n",
      "Epoch[57/70], Step[112800], Loss: 0.0453, lr: 0.000001\n",
      "Epoch[57/70], Step[112900], Loss: 0.0018, lr: 0.000001\n",
      "Epoch[57/70], Step[113000], Loss: 0.0251, lr: 0.000001\n",
      "Epoch[57/70], Step[113100], Loss: 0.0433, lr: 0.000001\n",
      "Epoch[57/70], Step[113200], Loss: 0.0084, lr: 0.000001\n",
      "Epoch[57/70], Step[113300], Loss: 0.0032, lr: 0.000001\n",
      "Epoch[57/70], Step[113400], Loss: 0.0144, lr: 0.000001\n",
      "Epoch[57/70], Step[113500], Loss: 0.0125, lr: 0.000001\n",
      "Epoch[57/70], Step[113600], Loss: 0.0124, lr: 0.000001\n",
      "Epoch[57/70], Step[113700], Loss: 0.0188, lr: 0.000001\n",
      "Epoch[57/70], Step[113800], Loss: 0.0098, lr: 0.000001\n",
      "Epoch[57/70], Step[113900], Loss: 0.0367, lr: 0.000001\n",
      "Epoch[58/70], Step[114000], Loss: 0.0614, lr: 0.000001\n",
      "Epoch[58/70], Step[114100], Loss: 0.0070, lr: 0.000001\n",
      "Epoch[58/70], Step[114200], Loss: 0.0171, lr: 0.000001\n",
      "Epoch[58/70], Step[114300], Loss: 0.0036, lr: 0.000001\n",
      "Epoch[58/70], Step[114400], Loss: 0.0033, lr: 0.000001\n",
      "Epoch[58/70], Step[114500], Loss: 0.0013, lr: 0.000001\n",
      "Epoch[58/70], Step[114600], Loss: 0.0169, lr: 0.000001\n",
      "Epoch[58/70], Step[114700], Loss: 0.0078, lr: 0.000001\n",
      "Epoch[58/70], Step[114800], Loss: 0.1395, lr: 0.000001\n",
      "Epoch[58/70], Step[114900], Loss: 0.0020, lr: 0.000001\n",
      "Epoch[58/70], Step[115000], Loss: 0.0060, lr: 0.000001\n",
      "Epoch[58/70], Step[115100], Loss: 0.0657, lr: 0.000001\n",
      "Epoch[58/70], Step[115200], Loss: 0.0196, lr: 0.000001\n",
      "Epoch[58/70], Step[115300], Loss: 0.0134, lr: 0.000001\n",
      "Epoch[58/70], Step[115400], Loss: 0.0008, lr: 0.000001\n",
      "Epoch[58/70], Step[115500], Loss: 0.0081, lr: 0.000001\n",
      "Epoch[58/70], Step[115600], Loss: 0.0156, lr: 0.000001\n",
      "Epoch[58/70], Step[115700], Loss: 0.0117, lr: 0.000001\n",
      "Epoch[58/70], Step[115800], Loss: 0.0771, lr: 0.000001\n",
      "Epoch[58/70], Step[115900], Loss: 0.0103, lr: 0.000001\n",
      "Epoch[59/70], Step[116000], Loss: 0.0464, lr: 0.000001\n",
      "Epoch[59/70], Step[116100], Loss: 0.0154, lr: 0.000001\n",
      "Epoch[59/70], Step[116200], Loss: 0.0027, lr: 0.000001\n",
      "Epoch[59/70], Step[116300], Loss: 0.0162, lr: 0.000001\n",
      "Epoch[59/70], Step[116400], Loss: 0.0227, lr: 0.000001\n",
      "Epoch[59/70], Step[116500], Loss: 0.0026, lr: 0.000001\n",
      "Epoch[59/70], Step[116600], Loss: 0.0035, lr: 0.000001\n",
      "Epoch[59/70], Step[116700], Loss: 0.0420, lr: 0.000001\n",
      "Epoch[59/70], Step[116800], Loss: 0.0006, lr: 0.000001\n",
      "Epoch[59/70], Step[116900], Loss: 0.0123, lr: 0.000001\n",
      "Epoch[59/70], Step[117000], Loss: 0.0252, lr: 0.000001\n",
      "Epoch[59/70], Step[117100], Loss: 0.0029, lr: 0.000001\n",
      "Epoch[59/70], Step[117200], Loss: 0.0038, lr: 0.000001\n",
      "Epoch[59/70], Step[117300], Loss: 0.0055, lr: 0.000001\n",
      "Epoch[59/70], Step[117400], Loss: 0.0161, lr: 0.000001\n",
      "Epoch[59/70], Step[117500], Loss: 0.0150, lr: 0.000001\n",
      "Epoch[59/70], Step[117600], Loss: 0.0183, lr: 0.000001\n",
      "Epoch[59/70], Step[117700], Loss: 0.0010, lr: 0.000001\n",
      "Epoch[59/70], Step[117800], Loss: 0.0813, lr: 0.000001\n",
      "Epoch[59/70], Step[117900], Loss: 0.0033, lr: 0.000001\n",
      "Epoch[60/70], Step[118000], Loss: 0.0441, lr: 0.000001\n",
      "Epoch[60/70], Step[118100], Loss: 0.0101, lr: 0.000001\n",
      "Epoch[60/70], Step[118200], Loss: 0.0060, lr: 0.000001\n",
      "Epoch[60/70], Step[118300], Loss: 0.0178, lr: 0.000001\n",
      "Epoch[60/70], Step[118400], Loss: 0.0235, lr: 0.000001\n",
      "Epoch[60/70], Step[118500], Loss: 0.0236, lr: 0.000001\n",
      "Epoch[60/70], Step[118600], Loss: 0.0015, lr: 0.000001\n",
      "Epoch[60/70], Step[118700], Loss: 0.0399, lr: 0.000001\n",
      "Epoch[60/70], Step[118800], Loss: 0.0853, lr: 0.000001\n",
      "Epoch[60/70], Step[118900], Loss: 0.0200, lr: 0.000001\n",
      "Epoch[60/70], Step[119000], Loss: 0.0249, lr: 0.000001\n",
      "Epoch[60/70], Step[119100], Loss: 0.0641, lr: 0.000001\n",
      "Epoch[60/70], Step[119200], Loss: 0.0107, lr: 0.000001\n",
      "Epoch[60/70], Step[119300], Loss: 0.0109, lr: 0.000001\n",
      "Epoch[60/70], Step[119400], Loss: 0.0099, lr: 0.000001\n",
      "Epoch[60/70], Step[119500], Loss: 0.0035, lr: 0.000001\n",
      "Epoch[60/70], Step[119600], Loss: 0.0080, lr: 0.000001\n",
      "Epoch[60/70], Step[119700], Loss: 0.0257, lr: 0.000001\n",
      "Epoch[60/70], Step[119800], Loss: 0.0079, lr: 0.000001\n",
      "Epoch[60/70], Step[119900], Loss: 0.0111, lr: 0.000001\n",
      "Epoch[61/70], Step[120000], Loss: 0.0208, lr: 0.000000\n",
      "Epoch[61/70], Step[120100], Loss: 0.0016, lr: 0.000000\n",
      "Epoch[61/70], Step[120200], Loss: 0.0026, lr: 0.000000\n",
      "Epoch[61/70], Step[120300], Loss: 0.0064, lr: 0.000000\n",
      "Epoch[61/70], Step[120400], Loss: 0.0111, lr: 0.000000\n",
      "Epoch[61/70], Step[120500], Loss: 0.0020, lr: 0.000000\n",
      "Epoch[61/70], Step[120600], Loss: 0.0043, lr: 0.000000\n",
      "Epoch[61/70], Step[120700], Loss: 0.0836, lr: 0.000000\n",
      "Epoch[61/70], Step[120800], Loss: 0.0184, lr: 0.000000\n",
      "Epoch[61/70], Step[120900], Loss: 0.0006, lr: 0.000000\n",
      "Epoch[61/70], Step[121000], Loss: 0.0182, lr: 0.000000\n",
      "Epoch[61/70], Step[121100], Loss: 0.0024, lr: 0.000000\n",
      "Epoch[61/70], Step[121200], Loss: 0.0018, lr: 0.000000\n",
      "Epoch[61/70], Step[121300], Loss: 0.0021, lr: 0.000000\n",
      "Epoch[61/70], Step[121400], Loss: 0.0170, lr: 0.000000\n",
      "Epoch[61/70], Step[121500], Loss: 0.0159, lr: 0.000000\n",
      "Epoch[61/70], Step[121600], Loss: 0.0035, lr: 0.000000\n",
      "Epoch[61/70], Step[121700], Loss: 0.0317, lr: 0.000000\n",
      "Epoch[61/70], Step[121800], Loss: 0.0048, lr: 0.000000\n",
      "Epoch[61/70], Step[121900], Loss: 0.0081, lr: 0.000000\n",
      "Epoch[62/70], Step[122000], Loss: 0.0444, lr: 0.000000\n",
      "Epoch[62/70], Step[122100], Loss: 0.0088, lr: 0.000000\n",
      "Epoch[62/70], Step[122200], Loss: 0.0191, lr: 0.000000\n",
      "Epoch[62/70], Step[122300], Loss: 0.0014, lr: 0.000000\n",
      "Epoch[62/70], Step[122400], Loss: 0.0015, lr: 0.000000\n",
      "Epoch[62/70], Step[122500], Loss: 0.0086, lr: 0.000000\n",
      "Epoch[62/70], Step[122600], Loss: 0.0026, lr: 0.000000\n",
      "Epoch[62/70], Step[122700], Loss: 0.0029, lr: 0.000000\n",
      "Epoch[62/70], Step[122800], Loss: 0.1574, lr: 0.000000\n",
      "Epoch[62/70], Step[122900], Loss: 0.0197, lr: 0.000000\n",
      "Epoch[62/70], Step[123000], Loss: 0.0065, lr: 0.000000\n",
      "Epoch[62/70], Step[123100], Loss: 0.0020, lr: 0.000000\n",
      "Epoch[62/70], Step[123200], Loss: 0.0080, lr: 0.000000\n",
      "Epoch[62/70], Step[123300], Loss: 0.0099, lr: 0.000000\n",
      "Epoch[62/70], Step[123400], Loss: 0.0101, lr: 0.000000\n",
      "Epoch[62/70], Step[123500], Loss: 0.0146, lr: 0.000000\n",
      "Epoch[62/70], Step[123600], Loss: 0.0020, lr: 0.000000\n",
      "Epoch[62/70], Step[123700], Loss: 0.0400, lr: 0.000000\n",
      "Epoch[62/70], Step[123800], Loss: 0.0496, lr: 0.000000\n",
      "Epoch[62/70], Step[123900], Loss: 0.0694, lr: 0.000000\n",
      "Epoch[63/70], Step[124000], Loss: 0.0881, lr: 0.000000\n",
      "Epoch[63/70], Step[124100], Loss: 0.0241, lr: 0.000000\n",
      "Epoch[63/70], Step[124200], Loss: 0.0031, lr: 0.000000\n",
      "Epoch[63/70], Step[124300], Loss: 0.0007, lr: 0.000000\n",
      "Epoch[63/70], Step[124400], Loss: 0.0011, lr: 0.000000\n",
      "Epoch[63/70], Step[124500], Loss: 0.0091, lr: 0.000000\n",
      "Epoch[63/70], Step[124600], Loss: 0.0477, lr: 0.000000\n",
      "Epoch[63/70], Step[124700], Loss: 0.0711, lr: 0.000000\n",
      "Epoch[63/70], Step[124800], Loss: 0.0515, lr: 0.000000\n",
      "Epoch[63/70], Step[124900], Loss: 0.0014, lr: 0.000000\n",
      "Epoch[63/70], Step[125000], Loss: 0.0023, lr: 0.000000\n",
      "Epoch[63/70], Step[125100], Loss: 0.0137, lr: 0.000000\n",
      "Epoch[63/70], Step[125200], Loss: 0.0857, lr: 0.000000\n",
      "Epoch[63/70], Step[125300], Loss: 0.0027, lr: 0.000000\n",
      "Epoch[63/70], Step[125400], Loss: 0.0018, lr: 0.000000\n",
      "Epoch[63/70], Step[125500], Loss: 0.0012, lr: 0.000000\n",
      "Epoch[63/70], Step[125600], Loss: 0.0046, lr: 0.000000\n",
      "Epoch[63/70], Step[125700], Loss: 0.0597, lr: 0.000000\n",
      "Epoch[63/70], Step[125800], Loss: 0.0513, lr: 0.000000\n",
      "Epoch[63/70], Step[125900], Loss: 0.0076, lr: 0.000000\n",
      "Epoch[64/70], Step[126000], Loss: 0.0207, lr: 0.000000\n",
      "Epoch[64/70], Step[126100], Loss: 0.0085, lr: 0.000000\n",
      "Epoch[64/70], Step[126200], Loss: 0.0009, lr: 0.000000\n",
      "Epoch[64/70], Step[126300], Loss: 0.0142, lr: 0.000000\n",
      "Epoch[64/70], Step[126400], Loss: 0.0213, lr: 0.000000\n",
      "Epoch[64/70], Step[126500], Loss: 0.0020, lr: 0.000000\n",
      "Epoch[64/70], Step[126600], Loss: 0.0225, lr: 0.000000\n",
      "Epoch[64/70], Step[126700], Loss: 0.0006, lr: 0.000000\n",
      "Epoch[64/70], Step[126800], Loss: 0.0553, lr: 0.000000\n",
      "Epoch[64/70], Step[126900], Loss: 0.0063, lr: 0.000000\n",
      "Epoch[64/70], Step[127000], Loss: 0.0030, lr: 0.000000\n",
      "Epoch[64/70], Step[127100], Loss: 0.0663, lr: 0.000000\n",
      "Epoch[64/70], Step[127200], Loss: 0.0138, lr: 0.000000\n",
      "Epoch[64/70], Step[127300], Loss: 0.0027, lr: 0.000000\n",
      "Epoch[64/70], Step[127400], Loss: 0.0066, lr: 0.000000\n",
      "Epoch[64/70], Step[127500], Loss: 0.0032, lr: 0.000000\n",
      "Epoch[64/70], Step[127600], Loss: 0.0033, lr: 0.000000\n",
      "Epoch[64/70], Step[127700], Loss: 0.0380, lr: 0.000000\n",
      "Epoch[64/70], Step[127800], Loss: 0.0636, lr: 0.000000\n",
      "Epoch[64/70], Step[127900], Loss: 0.0347, lr: 0.000000\n",
      "Epoch[65/70], Step[128000], Loss: 0.0124, lr: 0.000000\n",
      "Epoch[65/70], Step[128100], Loss: 0.0026, lr: 0.000000\n",
      "Epoch[65/70], Step[128200], Loss: 0.0016, lr: 0.000000\n",
      "Epoch[65/70], Step[128300], Loss: 0.0080, lr: 0.000000\n",
      "Epoch[65/70], Step[128400], Loss: 0.0034, lr: 0.000000\n",
      "Epoch[65/70], Step[128500], Loss: 0.0163, lr: 0.000000\n",
      "Epoch[65/70], Step[128600], Loss: 0.0047, lr: 0.000000\n",
      "Epoch[65/70], Step[128700], Loss: 0.0471, lr: 0.000000\n",
      "Epoch[65/70], Step[128800], Loss: 0.0819, lr: 0.000000\n",
      "Epoch[65/70], Step[128900], Loss: 0.0109, lr: 0.000000\n",
      "Epoch[65/70], Step[129000], Loss: 0.0265, lr: 0.000000\n",
      "Epoch[65/70], Step[129100], Loss: 0.0541, lr: 0.000000\n",
      "Epoch[65/70], Step[129200], Loss: 0.0128, lr: 0.000000\n",
      "Epoch[65/70], Step[129300], Loss: 0.0148, lr: 0.000000\n",
      "Epoch[65/70], Step[129400], Loss: 0.0020, lr: 0.000000\n",
      "Epoch[65/70], Step[129500], Loss: 0.0032, lr: 0.000000\n",
      "Epoch[65/70], Step[129600], Loss: 0.0142, lr: 0.000000\n",
      "Epoch[65/70], Step[129700], Loss: 0.0013, lr: 0.000000\n",
      "Epoch[65/70], Step[129800], Loss: 0.0091, lr: 0.000000\n",
      "Epoch[65/70], Step[129900], Loss: 0.0357, lr: 0.000000\n",
      "Epoch[66/70], Step[130000], Loss: 0.0030, lr: 0.000000\n",
      "Epoch[66/70], Step[130100], Loss: 0.0015, lr: 0.000000\n",
      "Epoch[66/70], Step[130200], Loss: 0.0024, lr: 0.000000\n",
      "Epoch[66/70], Step[130300], Loss: 0.0002, lr: 0.000000\n",
      "Epoch[66/70], Step[130400], Loss: 0.0021, lr: 0.000000\n",
      "Epoch[66/70], Step[130500], Loss: 0.0016, lr: 0.000000\n",
      "Epoch[66/70], Step[130600], Loss: 0.0089, lr: 0.000000\n",
      "Epoch[66/70], Step[130700], Loss: 0.0034, lr: 0.000000\n",
      "Epoch[66/70], Step[130800], Loss: 0.0182, lr: 0.000000\n",
      "Epoch[66/70], Step[130900], Loss: 0.0032, lr: 0.000000\n",
      "Epoch[66/70], Step[131000], Loss: 0.0136, lr: 0.000000\n",
      "Epoch[66/70], Step[131100], Loss: 0.0127, lr: 0.000000\n",
      "Epoch[66/70], Step[131200], Loss: 0.0136, lr: 0.000000\n",
      "Epoch[66/70], Step[131300], Loss: 0.0063, lr: 0.000000\n",
      "Epoch[66/70], Step[131400], Loss: 0.0166, lr: 0.000000\n",
      "Epoch[66/70], Step[131500], Loss: 0.0046, lr: 0.000000\n",
      "Epoch[66/70], Step[131600], Loss: 0.0112, lr: 0.000000\n",
      "Epoch[66/70], Step[131700], Loss: 0.0278, lr: 0.000000\n",
      "Epoch[66/70], Step[131800], Loss: 0.0172, lr: 0.000000\n",
      "Epoch[66/70], Step[131900], Loss: 0.0178, lr: 0.000000\n",
      "Epoch[67/70], Step[132000], Loss: 0.0322, lr: 0.000000\n",
      "Epoch[67/70], Step[132100], Loss: 0.0028, lr: 0.000000\n",
      "Epoch[67/70], Step[132200], Loss: 0.0178, lr: 0.000000\n",
      "Epoch[67/70], Step[132300], Loss: 0.0004, lr: 0.000000\n",
      "Epoch[67/70], Step[132400], Loss: 0.0018, lr: 0.000000\n",
      "Epoch[67/70], Step[132500], Loss: 0.0015, lr: 0.000000\n",
      "Epoch[67/70], Step[132600], Loss: 0.0184, lr: 0.000000\n",
      "Epoch[67/70], Step[132700], Loss: 0.0335, lr: 0.000000\n",
      "Epoch[67/70], Step[132800], Loss: 0.0944, lr: 0.000000\n",
      "Epoch[67/70], Step[132900], Loss: 0.0103, lr: 0.000000\n",
      "Epoch[67/70], Step[133000], Loss: 0.0141, lr: 0.000000\n",
      "Epoch[67/70], Step[133100], Loss: 0.0466, lr: 0.000000\n",
      "Epoch[67/70], Step[133200], Loss: 0.0452, lr: 0.000000\n",
      "Epoch[67/70], Step[133300], Loss: 0.0009, lr: 0.000000\n",
      "Epoch[67/70], Step[133400], Loss: 0.0131, lr: 0.000000\n",
      "Epoch[67/70], Step[133500], Loss: 0.0029, lr: 0.000000\n",
      "Epoch[67/70], Step[133600], Loss: 0.0099, lr: 0.000000\n",
      "Epoch[67/70], Step[133700], Loss: 0.0091, lr: 0.000000\n",
      "Epoch[67/70], Step[133800], Loss: 0.0148, lr: 0.000000\n",
      "Epoch[67/70], Step[133900], Loss: 0.0034, lr: 0.000000\n",
      "Epoch[68/70], Step[134000], Loss: 0.0116, lr: 0.000000\n",
      "Epoch[68/70], Step[134100], Loss: 0.0226, lr: 0.000000\n",
      "Epoch[68/70], Step[134200], Loss: 0.0033, lr: 0.000000\n",
      "Epoch[68/70], Step[134300], Loss: 0.0004, lr: 0.000000\n",
      "Epoch[68/70], Step[134400], Loss: 0.0155, lr: 0.000000\n",
      "Epoch[68/70], Step[134500], Loss: 0.0180, lr: 0.000000\n",
      "Epoch[68/70], Step[134600], Loss: 0.0055, lr: 0.000000\n",
      "Epoch[68/70], Step[134700], Loss: 0.0442, lr: 0.000000\n",
      "Epoch[68/70], Step[134800], Loss: 0.0911, lr: 0.000000\n",
      "Epoch[68/70], Step[134900], Loss: 0.0167, lr: 0.000000\n",
      "Epoch[68/70], Step[135000], Loss: 0.0177, lr: 0.000000\n",
      "Epoch[68/70], Step[135100], Loss: 0.0171, lr: 0.000000\n",
      "Epoch[68/70], Step[135200], Loss: 0.0803, lr: 0.000000\n",
      "Epoch[68/70], Step[135300], Loss: 0.0083, lr: 0.000000\n",
      "Epoch[68/70], Step[135400], Loss: 0.0161, lr: 0.000000\n",
      "Epoch[68/70], Step[135500], Loss: 0.0054, lr: 0.000000\n",
      "Epoch[68/70], Step[135600], Loss: 0.0066, lr: 0.000000\n",
      "Epoch[68/70], Step[135700], Loss: 0.0173, lr: 0.000000\n",
      "Epoch[68/70], Step[135800], Loss: 0.0800, lr: 0.000000\n",
      "Epoch[68/70], Step[135900], Loss: 0.0713, lr: 0.000000\n",
      "Epoch[69/70], Step[136000], Loss: 0.0021, lr: 0.000000\n",
      "Epoch[69/70], Step[136100], Loss: 0.0131, lr: 0.000000\n",
      "Epoch[69/70], Step[136200], Loss: 0.0248, lr: 0.000000\n",
      "Epoch[69/70], Step[136300], Loss: 0.0183, lr: 0.000000\n",
      "Epoch[69/70], Step[136400], Loss: 0.0075, lr: 0.000000\n",
      "Epoch[69/70], Step[136500], Loss: 0.0083, lr: 0.000000\n",
      "Epoch[69/70], Step[136600], Loss: 0.0920, lr: 0.000000\n",
      "Epoch[69/70], Step[136700], Loss: 0.0215, lr: 0.000000\n",
      "Epoch[69/70], Step[136800], Loss: 0.0648, lr: 0.000000\n",
      "Epoch[69/70], Step[136900], Loss: 0.0075, lr: 0.000000\n",
      "Epoch[69/70], Step[137000], Loss: 0.0172, lr: 0.000000\n",
      "Epoch[69/70], Step[137100], Loss: 0.0048, lr: 0.000000\n",
      "Epoch[69/70], Step[137200], Loss: 0.0206, lr: 0.000000\n",
      "Epoch[69/70], Step[137300], Loss: 0.0011, lr: 0.000000\n",
      "Epoch[69/70], Step[137400], Loss: 0.0018, lr: 0.000000\n",
      "Epoch[69/70], Step[137500], Loss: 0.0160, lr: 0.000000\n",
      "Epoch[69/70], Step[137600], Loss: 0.0146, lr: 0.000000\n",
      "Epoch[69/70], Step[137700], Loss: 0.0777, lr: 0.000000\n",
      "Epoch[69/70], Step[137800], Loss: 0.0745, lr: 0.000000\n",
      "Epoch[69/70], Step[137900], Loss: 0.0006, lr: 0.000000\n",
      "Epoch[70/70], Step[138000], Loss: 0.0391, lr: 0.000000\n",
      "Epoch[70/70], Step[138100], Loss: 0.0291, lr: 0.000000\n",
      "Epoch[70/70], Step[138200], Loss: 0.0231, lr: 0.000000\n",
      "Epoch[70/70], Step[138300], Loss: 0.0078, lr: 0.000000\n",
      "Epoch[70/70], Step[138400], Loss: 0.0117, lr: 0.000000\n",
      "Epoch[70/70], Step[138500], Loss: 0.0003, lr: 0.000000\n",
      "Epoch[70/70], Step[138600], Loss: 0.0649, lr: 0.000000\n",
      "Epoch[70/70], Step[138700], Loss: 0.0010, lr: 0.000000\n",
      "Epoch[70/70], Step[138800], Loss: 0.1631, lr: 0.000000\n",
      "Epoch[70/70], Step[138900], Loss: 0.0028, lr: 0.000000\n",
      "Epoch[70/70], Step[139000], Loss: 0.0008, lr: 0.000000\n",
      "Epoch[70/70], Step[139100], Loss: 0.0542, lr: 0.000000\n",
      "Epoch[70/70], Step[139200], Loss: 0.0020, lr: 0.000000\n",
      "Epoch[70/70], Step[139300], Loss: 0.0112, lr: 0.000000\n",
      "Epoch[70/70], Step[139400], Loss: 0.0036, lr: 0.000000\n",
      "Epoch[70/70], Step[139500], Loss: 0.0017, lr: 0.000000\n",
      "Epoch[70/70], Step[139600], Loss: 0.0111, lr: 0.000000\n",
      "Epoch[70/70], Step[139700], Loss: 0.0928, lr: 0.000000\n",
      "Epoch[70/70], Step[139800], Loss: 0.0671, lr: 0.000000\n",
      "Epoch[70/70], Step[139900], Loss: 0.0689, lr: 0.000000\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "batch_size = 2\n",
    "model_ema_steps = 10\n",
    "num_epochs = 70\n",
    "model_ema_decay = 0.995\n",
    "\n",
    "# Define the model\n",
    "model = Diffusion_Control(\n",
    "    timesteps=1000,\n",
    "    image_size=64,\n",
    "    in_channels=1,\n",
    "    base_dim=32,\n",
    "    dim_mults=[1, 2, 4, 8]\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=100,\n",
    "    num_training_steps=(len(voxel_dataloader) * num_epochs),\n",
    ")\n",
    "adjust = 1 * batch_size * model_ema_steps / num_epochs\n",
    "alpha = 1.0 - model_ema_decay\n",
    "alpha = min(1.0, alpha * adjust)\n",
    "model_ema = ExponentialMovingAverage(model, device=device, decay=1.0 - alpha)\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "min_loss = np.inf\n",
    "global_steps = 0\n",
    "\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "checkpoint_path = \"condition_results/steps_00140000.pt\"\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "model_ema.load_state_dict(checkpoint[\"model_ema\"])\n",
    "# Load checkpoint if exists\n",
    "# if os.path.exists(checkpoint_path):\n",
    "#     print(\"Loading best checkpoint...\")\n",
    "#     checkpoint = torch.load(checkpoint_path)\n",
    "#     unet_state_dict = {k.replace(\"model.\", \" \"): v for k, v in checkpoint['model'].items() if k.startswith(\"model.\")}\n",
    "#     unet_state_dict = {k.strip(): v for k, v in unet_state_dict.items() if k.startswith(\" \")}\n",
    "\n",
    "#     model.unet.load_state_dict(unet_state_dict)  # Load UNet parameters only\n",
    "#     # model_ema.load_state_dict(ema_state_dict)  \n",
    "#     print(\"Checkpoint loaded successfully!\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for (voxel_batch, condition_batch) in zip(voxel_dataloader, condition_dataloader):\n",
    "        # Prepare inputs\n",
    "        noise = torch.randn_like(voxel_batch).to(device)\n",
    "        voxel_batch = voxel_batch.to(device)\n",
    "        condition_batch = condition_batch.to(device)\n",
    "\n",
    "        # Model forward pass\n",
    "\n",
    "        pred = model(voxel_batch,condition_batch, noise)  # Ensure model supports conditioning\n",
    "        noise = noise.unsqueeze(1)\n",
    "        loss = loss_fn(pred, noise)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Update EMA\n",
    "        if global_steps % model_ema_steps == 0:\n",
    "            model_ema.update_parameters(model)\n",
    "\n",
    "        # Logging\n",
    "        if global_steps % 100 == 0:\n",
    "            print(f\"Epoch[{epoch + 1}/{num_epochs}], Step[{global_steps}], Loss: {loss.item():.4f}, lr: {lr_scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if loss.item() < min_loss and epoch > 1:\n",
    "            min_loss = loss.item()\n",
    "            torch.save(\n",
    "                {\"model\": model.state_dict(), \"model_ema\": model_ema.state_dict()},\n",
    "                f\"results/best.pt\"\n",
    "            )\n",
    "        global_steps += 1\n",
    "\n",
    "    # Save checkpoint\n",
    "    torch.save(\n",
    "        {\"model\": model.state_dict(), \"model_ema\": model_ema.state_dict()},\n",
    "        f\"condition_results/steps_{global_steps:08d}.pt\"\n",
    "    )\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Loading best checkpoint...\n",
      "Checkpoint loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 1000/1000 [00:32<00:00, 30.68it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:32<00:00, 30.61it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:32<00:00, 30.68it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:32<00:00, 30.60it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:32<00:00, 30.61it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:32<00:00, 30.57it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:32<00:00, 30.47it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:32<00:00, 30.50it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:32<00:00, 30.36it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:32<00:00, 30.46it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:32<00:00, 30.42it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:32<00:00, 30.44it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:32<00:00, 30.37it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:32<00:00, 30.62it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:32<00:00, 30.59it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:32<00:00, 30.65it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:32<00:00, 30.53it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:32<00:00, 30.63it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:32<00:00, 30.58it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:32<00:00, 30.67it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:32<00:00, 30.55it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:32<00:00, 30.65it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:32<00:00, 30.66it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:32<00:00, 30.60it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:32<00:00, 30.60it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:32<00:00, 30.66it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:31<00:00, 32.23it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:30<00:00, 32.94it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:30<00:00, 32.50it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:31<00:00, 31.84it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:30<00:00, 32.39it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:30<00:00, 32.30it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:30<00:00, 32.80it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:30<00:00, 32.56it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:30<00:00, 32.44it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:30<00:00, 32.50it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:31<00:00, 31.98it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:31<00:00, 31.43it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:31<00:00, 31.82it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:30<00:00, 32.68it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:30<00:00, 32.44it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:31<00:00, 31.70it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:31<00:00, 31.70it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:31<00:00, 31.88it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:30<00:00, 32.96it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:31<00:00, 32.00it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:31<00:00, 32.25it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:30<00:00, 32.35it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:30<00:00, 32.54it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:30<00:00, 32.59it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:31<00:00, 32.22it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:30<00:00, 32.36it/s]\n",
      "Sampling: 100%|██████████| 1000/1000 [00:30<00:00, 32.87it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Paths and directories\n",
    "checkpoint_path = \"condition_results/steps_00140000.pt\"\n",
    "save_dir = \"generated_samples\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Load the trained model and EMA\n",
    "model = Diffusion_Control(\n",
    "    timesteps=1000,\n",
    "    image_size=64,\n",
    "    in_channels=1,\n",
    "    base_dim=32,\n",
    "    dim_mults=[1, 2, 4, 8]\n",
    ").to(device)\n",
    "\n",
    "model_ema = ExponentialMovingAverage(model, device=device, decay=1.0 - 0.995)\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(\"Loading best checkpoint...\")\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "    model_ema.load_state_dict(checkpoint[\"model_ema\"])\n",
    "    print(\"Checkpoint loaded successfully!\")\n",
    "\n",
    "model.eval()  # Switch to evaluation mode\n",
    "model_ema.eval()\n",
    "\n",
    "# Data loader for control dataset\n",
    "# control_dataset_loader = ...  # Define your control dataset loader here\n",
    "condition_dataloader = DataLoader(condition_dataset, batch_size=1, shuffle=True)\n",
    "samples = []\n",
    "with torch.no_grad():\n",
    "    for i, condition_batch in enumerate(condition_dataloader):\n",
    "        condition_batch = condition_batch.to(device)\n",
    "\n",
    "        # Generate noise for sampling\n",
    "        batch_size = condition_batch.size(0)\n",
    "        noise = torch.randn(1, 1, 64, 64,64).to(device)\n",
    "\n",
    "        # Perform sampling with the model\n",
    "        generated_samples = model_ema.module.sampling(batch_size, condition_batch, noise)\n",
    "        \n",
    "        # Process samples\n",
    "        for j in range(batch_size):\n",
    "            voxel_1d_array = generated_samples[j].cpu().numpy()\n",
    "\n",
    "            # Convert to binary voxel data\n",
    "            binary_data = (voxel_1d_array > 0.5).astype(int)\n",
    "\n",
    "            # Parameters for voxel map\n",
    "            voxel_size = 0.25\n",
    "            grid_size = 64\n",
    "\n",
    "            # Reshape binary data into 8x8x8\n",
    "            reshaped_data = binary_data.reshape(grid_size, 1, grid_size, 1, grid_size, 1).mean(axis=(1, 3, 5))\n",
    "            voxel_data = (reshaped_data > 0.5).astype(int)\n",
    "\n",
    "            # Prepare the 3D plot\n",
    "            fig = plt.figure(figsize=(10, 10))\n",
    "            ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "            # Create a 3D grid for the voxel map\n",
    "            x, y, z = np.indices((grid_size + 1, grid_size + 1, grid_size + 1)) * voxel_size\n",
    "\n",
    "            # Display voxels\n",
    "            filled_voxels = (voxel_data == 1)\n",
    "            ax.voxels(x, y, z, filled_voxels, facecolors=\"blue\", edgecolors=\"black\", alpha=0.7)\n",
    "\n",
    "            # Set labels and aspect ratio\n",
    "            ax.set_xlabel('X')\n",
    "            ax.set_ylabel('Y')\n",
    "            ax.set_zlabel('Z')\n",
    "            ax.set_aspect('auto')\n",
    "            plt.title(\"8x8x8 Voxel Map\")\n",
    "\n",
    "            # Save the plot\n",
    "            save_path = os.path.join(save_dir, f\"sample_{i * batch_size + j + 1}.png\")\n",
    "            plt.savefig(save_path)\n",
    "            plt.close(fig)\n",
    "\n",
    "        # Break loop after generating desired number of samples\n",
    "        if len(samples) >= 2:\n",
    "            break\n",
    "\n",
    "print(\"Generation complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
